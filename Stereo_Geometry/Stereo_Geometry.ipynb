{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stereo Geometry\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Understanding stereo geometry is a fundamental step in 3D computer vision. It provides\n",
    "insight into the relationship between captured images and the camera's position. In this\n",
    "tutorial, we will explore the calibration of a camera, estimating the camera projection\n",
    "matrix to establish the link between 2D image points and 3D world points. Additionally,\n",
    "we will delve into the concept of the fundamental matrix, which connects two stereo\n",
    "images and aids in identifying epipolar lines. Lastly, we will rectify or warp stereo\n",
    "images to align them along the y-axis.\n",
    "\n",
    "To start, let's familiarize ourselves with the data we will be working with:\n",
    "\n",
    "-   `pic_a.jpg` and `pic_b.jpg`: These are the left and right images.\n",
    "\n",
    "-   `pts2d-norm-pic_a.txt`: This file contains normalized 2D points in the left image,\n",
    "    which we can use for initial verification.\n",
    "\n",
    "-   `pts3d-norm.txt`: Normalized 3D points in world coordinates, also for initial\n",
    "    verification.\n",
    "\n",
    "-   `pts2d-pic_a.txt` and `pts2d-pic_b.txt`: Unnormalized 2D points in the left and\n",
    "    right images.\n",
    "\n",
    "-   `pts3d.txt`: Unnormalized 3D points in world coordinates.\n",
    "\n",
    "**Important Note on Implementation of this Notebook**\n",
    "\n",
    "-   In this notebook, we strive to follow the implementation outlined in the book\n",
    "    \"**Multiple View Geometry in Computer Vision (MVG)**.\" Therefore, there may be\n",
    "    slight inconsistencies between the input data and the textbook. For instance, the\n",
    "    average distance from the origin of the normalized 2D points in\n",
    "    `pts2d-norm-pic_a.txt` is `0.7785`, whereas it is suggested to be $\\sqrt{2}$ in MVG.\n",
    "    Similarly, for 3D points, the average distance is `1.2147`, whereas MVG suggests\n",
    "    $\\sqrt{3}$. We adhere to the MVG guidelines unless there are compelling reasons not\n",
    "    to.\n",
    "\n",
    "**RULES:** As usual, **`OpenCV`** is banned in this repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "from stereo_geometry_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from skimage.transform import warp, ProjectiveTransform\n",
    "\n",
    "left_img = io.imread(\"./input/pic_a.jpg\")\n",
    "right_img = io.imread(\"./input/pic_b.jpg\")\n",
    "\n",
    "orig_norm_pts2d = read_keypoints(\"./input/pts2d-norm-pic_a.txt\")\n",
    "orig_norm_pts3d = read_keypoints(\"./input/pts3d-norm.txt\")\n",
    "\n",
    "pts2d_left = read_keypoints(\"./input/pts2d-pic_a.txt\")\n",
    "pts2d_right = read_keypoints(\"./input/pts2d-pic_b.txt\")\n",
    "pts3d = read_keypoints(\"./input/pts3d.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "The files `pts2d-pic_a.txt` and `pts3d.txt` contain a list of twenty 2D and 3D points\n",
    "corresponding to the image `pic_a.jpg`. Our objective is to compute the projection\n",
    "matrix that transforms 3D world coordinates into 2D image coordinates. Using homogeneous\n",
    "coordinates, the equation is expressed as follows:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "u \\\\\n",
    "v \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\simeq\n",
    "\\begin{bmatrix}\n",
    "s*u \\\\\n",
    "s*v \\\\\n",
    "s \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "m_{11} & m_{12} & m_{13} & m_{14} \\\\\n",
    "m_{21} & m_{22} & m_{23} & m_{24} \\\\\n",
    "m_{31} & m_{32} & m_{33} & m_{34} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "X \\\\\n",
    "Y \\\\\n",
    "Z \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To find the 3x4 matrix M, we can either solve the homogeneous version of these equations\n",
    "using Singular Value Decomposition (SVD) or set $m_{34}$ to 1 and employ a normal least\n",
    "squares method. It's important to note that M is known only up to a scale factor.\n",
    "\n",
    "<figure>\n",
    "  <div style=\"display: flex; justify-content: space-between;\">\n",
    "    <div style=\"text-align: center;\">\n",
    "      <img src=\"./input/pic_a.jpg\" style=\"width: 80%;\" alt=\"Left Image\">\n",
    "      <p><strong>Unrectified Left Image \"pic_a.jpg\"</strong></p>\n",
    "    </div>\n",
    "    <div style=\"text-align: center;\">\n",
    "      <img src=\"./input/pic_b.jpg\" style=\"width: 80%;\" alt=\"Right Image\">\n",
    "      <p><strong>Unrectified Right Image \"pic_b.jpg\"</strong></p>\n",
    "    </div>\n",
    "  </div>\n",
    "</figure>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "As per the guidelines in MVG, the normalization of 2D and 3D points is a critical step.\n",
    "In this notebook, we adopt $\\sqrt{2}$ as the average distance from the origin for 2D\n",
    "points and $\\sqrt{3}$ as the average distance from the origin for 3D points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before normalization:\")\n",
    "print(f\"The unnormalized 2D point: {pts2d_left[:, 0]}\")\n",
    "print(f\"The unnormalized 3D point: {pts3d[:, 0]}\")\n",
    "print(\"\\n\")\n",
    "print(\"Applying normalization ...\")\n",
    "norm_pts2d_left, sim_mat_T_l = normalize_image_points(\n",
    "    pts2d_left, avg_distance=np.sqrt(2)\n",
    ")\n",
    "norm_pts2d_right, sim_mat_T_r = normalize_image_points(\n",
    "    pts2d_right, avg_distance=np.sqrt(2)\n",
    ")\n",
    "norm_pts3d, sim_mat_U = normalize_space_points(pts3d, avg_distance=np.sqrt(3))\n",
    "print(f\"The average distance to origin is:\")\n",
    "print(f\"2D: {average_distance_to_origin(norm_pts2d_left)}\")\n",
    "print(f\"3D: {average_distance_to_origin(norm_pts3d)}\")\n",
    "print(\"\\n\")\n",
    "print(f\"After normalization:\")\n",
    "inverse_pts2d = np.dot(np.linalg.inv(sim_mat_T_l), norm_pts2d_left)\n",
    "inverse_pts3d = np.dot(np.linalg.inv(sim_mat_U), norm_pts3d)\n",
    "np.divide(inverse_pts2d, inverse_pts2d[-1, :], out=inverse_pts2d)\n",
    "np.divide(inverse_pts3d, inverse_pts3d[-1, :], out=inverse_pts3d)\n",
    "print(\n",
    "    \"We can verify the correctness of normalization by applying the inverse of similarity transform to the normalized points.\"\n",
    ")\n",
    "print(f\"The denormalized 2D point: {inverse_pts2d[:, 0]}\")\n",
    "print(f\"The denormalized 3D point: {inverse_pts3d[:, 0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection Matrix Estimation\n",
    "\n",
    "To estimate the projection matrix, we will employ Algorithm 7.1 from MVG's book, as\n",
    "illustrated below. This algorithm initially leverages a linear solution to generate an\n",
    "initial projection matrix estimate, which is subsequently refined using a least-squares\n",
    "method to minimize geometric error. The geometric error quantifies the disparity between\n",
    "the projected 2D points derived from 3D points and the actual 2D points extracted from\n",
    "the input data. This residual is computed as the square root of the sum of squared\n",
    "differences in $u$ and $v$.\n",
    "\n",
    "<img src=\"images/MVG-algo_7.1.png\" style=\"width:512px;\">\n",
    "\n",
    "To validate the correctness of our code, we can use a set of \"normalized points\" stored\n",
    "in the files `pts2d-norm-pic_a.txt` and `pts3d-norm.txt`. When we solve for $M$ using\n",
    "all these points, we should obtain a matrix that is a scaled equivalent of the\n",
    "following:\n",
    "\n",
    "$$\n",
    "M_{norm}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-0.4583 &  0.2947 & 0.0139 & -0.0040 \\\\\n",
    " 0.0509 &  0.0546 & 0.5410 &  0.0524 \\\\\n",
    "-0.1090 & -0.1784 & 0.0443 & -0.5968 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For example, when given a normalized 3D point\n",
    "${\\begin{bmatrix}1.2323 & 1.4421 & 0.4506 & 1.0\\end{bmatrix}}^T$, $M_{\\text{norm}}$\n",
    "will project this point to ${\\begin{bmatrix}u & v\\end{bmatrix}}^T$ of\n",
    "${\\begin{bmatrix}0.1419 & -0.4518\\end{bmatrix}}^T$ where we convert the homogeneous 2D\n",
    "point ${\\begin{bmatrix}us & vs & s\\end{bmatrix}}^T$ to its inhomogeneous version by\n",
    "dividing by 's', resulting in the actual transformed pixel coordinates in the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_linear_transformation(pts2d: np.ndarray, pts3d: np.ndarray):\n",
    "    # pts2d: (3, N), [u, v, 1]\n",
    "    # pts3d: (4, N), [X, Y, Z, 1]\n",
    "    assert pts2d.shape[0] == 3 and pts3d.shape[0] == 4\n",
    "    assert pts2d.shape[1] == pts3d.shape[1]\n",
    "    num_pts = pts2d.shape[1]\n",
    "    A = np.zeros((2 * num_pts, 12))\n",
    "    A[0::2, 0] = pts3d[0, :]\n",
    "    A[0::2, 1] = pts3d[1, :]\n",
    "    A[0::2, 2] = pts3d[2, :]\n",
    "    A[0::2, 3] = 1\n",
    "\n",
    "    A[1::2, 4] = pts3d[0, :]\n",
    "    A[1::2, 5] = pts3d[1, :]\n",
    "    A[1::2, 6] = pts3d[2, :]\n",
    "    A[1::2, 7] = 1\n",
    "\n",
    "    A[0::2, 8] = -1 * np.multiply(pts2d[0, :], pts3d[0, :])\n",
    "    A[1::2, 8] = -1 * np.multiply(pts2d[1, :], pts3d[0, :])\n",
    "    A[0::2, 9] = -1 * np.multiply(pts2d[0, :], pts3d[1, :])\n",
    "    A[1::2, 9] = -1 * np.multiply(pts2d[1, :], pts3d[1, :])\n",
    "    A[0::2, 10] = -1 * np.multiply(pts2d[0, :], pts3d[2, :])\n",
    "    A[1::2, 10] = -1 * np.multiply(pts2d[1, :], pts3d[2, :])\n",
    "    A[0::2, 11] = -1 * pts2d[0, :]\n",
    "    A[1::2, 11] = -1 * pts2d[1, :]\n",
    "\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(np.dot(A.T, A))\n",
    "    min_arg = np.argmin(eigenvalues)\n",
    "    return eigenvectors[:, min_arg]\n",
    "\n",
    "\n",
    "def minimize_geometric_error(m, pts2d, pts3d, dist=1):\n",
    "    # m: (12,)\n",
    "    # norm_pts2d: (3, N)\n",
    "    # norm_pts3d: (4, N)\n",
    "    # Does `dist` matter?\n",
    "    num_points = pts3d.shape[1]\n",
    "    M = np.reshape(m, (3, 4))\n",
    "    assert pts2d.shape[0] == 3 and pts2d.shape[1] == num_points\n",
    "    assert pts3d.shape[0] == 4 and pts3d.shape[1] == num_points\n",
    "\n",
    "    projected_2dpts = np.dot(M, pts3d)  # (3,4) x (4,N) -> (3,N)\n",
    "    projected_2dpts = dist * np.divide(projected_2dpts, projected_2dpts[2, :])\n",
    "    return np.sum((pts2d - projected_2dpts) ** 2)  # (3,N) -> scalar\n",
    "\n",
    "\n",
    "def generate_projection_matrix(norm_pts2d, norm_pts3d):\n",
    "    m0 = direct_linear_transformation(norm_pts2d, norm_pts3d)\n",
    "    res_1 = least_squares(\n",
    "        minimize_geometric_error, m0, args=(norm_pts2d, norm_pts3d)\n",
    "    )\n",
    "    return np.reshape(res_1.x, (3, 4)), res_1.cost\n",
    "\n",
    "\n",
    "proj_m, res = generate_projection_matrix(orig_norm_pts2d, orig_norm_pts3d)\n",
    "\n",
    "print(\"Using normalized 2D and 3D points given by tasks to estimate projection matrix.\\n\")\n",
    "print(f\"Estimated Projection Matrix: \\n{proj_m}\\n\")\n",
    "print(f\"Normalized Projection Matrix: \\n{proj_m / proj_m[2, 3]}\\n\")\n",
    "print(f\"Residual: {res}\")\n",
    "\n",
    "testing_point = np.array([1.2323, 1.4421, 0.4506, 1]).T\n",
    "projected_point = np.dot(proj_m, testing_point)\n",
    "print(projected_point, projected_point / projected_point[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "Indeed, the results are promising. When utilizing the provided normalized 2D and 3D\n",
    "points, the estimated projection matrix closely approximates the ground truth projection\n",
    "matrix provided in the task. Furthermore, when we apply this projection matrix to\n",
    "project a 3D point from world coordinates to image coordinates, the resulting 2D point\n",
    "aligns closely with the ground truth 2D point supplied in the task. These outcomes\n",
    "validate the accuracy and effectiveness of the projection matrix estimation and\n",
    "transformation process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Center Estimation\n",
    "\n",
    "Once we have the camera projection matrix, we can estimate the camera center using the\n",
    "following equations. Given the projection matrix $M$:\n",
    "\n",
    "$$\n",
    "M = [\\mathbf{Q}|\\mathbf{b}]\n",
    "$$\n",
    "\n",
    "The camera center $C$ can be calculated as:\n",
    "\n",
    "$$\n",
    "C =\n",
    "\\begin{bmatrix}\n",
    "    -\\mathbf{Q}^{-1}\\mathbf{b} \\\\\n",
    "    1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For debugging purposes, if we use the normalized 3D points to obtain the previously\n",
    "mentioned projection matrix, we would compute a camera center of:\n",
    "\n",
    "$$\n",
    "C_{norm} = {\\begin{bmatrix}-1.5125 & -2.3515 & 0.2826\\end{bmatrix}}^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera_center(m):\n",
    "    assert m.shape == (3, 4)\n",
    "    Q = m[:, :3]\n",
    "    b = m[:, 3]\n",
    "    return -1 * np.dot(np.linalg.inv(Q), b)\n",
    "\n",
    "\n",
    "print(\"The camera center derived from normalized 2D and 3D points is\", get_camera_center(proj_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying to Unnormalized 2D and 3D Points\n",
    "\n",
    "Following the implementation of the projection matrix function and its validation with\n",
    "normalized 2D and 3D points, we can extend these functions to handle unnormalized 2D and\n",
    "3D points. However, one additional function, `denormalize_projection_matrix`, is\n",
    "required. This function will enable the conversion of the camera matrix from normalized\n",
    "coordinates to the original unnormalized coordinates. Once we have this function, we can\n",
    "proceed to process our unnormalized 2D and 3D points using the following code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_projection_matrix(T, P, U):\n",
    "    return np.dot(np.linalg.inv(T), np.dot(np.reshape(P, (3, 4)), U))\n",
    "\n",
    "\n",
    "proj_m, res = generate_projection_matrix(norm_pts2d_left, norm_pts3d)\n",
    "print(\n",
    "    \"Using unnormalized 2D and 3D points given by tasks to estimate projection matrix.\\n\"\n",
    ")\n",
    "print(f\"Estimated Projection Matrix: \\n{proj_m}\\n\")\n",
    "print(f\"Normalized Projection Matrix: \\n{proj_m / proj_m[2, 3]}\\n\")\n",
    "print(f\"Residual: {res}\\n\")\n",
    "\n",
    "proj_m = denormalize_projection_matrix(sim_mat_T_l, proj_m, sim_mat_U)\n",
    "print(\n",
    "    \"The camera center derived from normalized 2D and 3D points is\",\n",
    "    get_camera_center(proj_m),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "As observed, the results demonstrate that the unnormalized and normalized projection\n",
    "matrices are not identical when applied to the normalized points. This outcome is\n",
    "entirely expected because, even though the camera projections differ, the underlying\n",
    "intrinsic, extrinsic parameters, and camera center remain consistent. For instance, the\n",
    "camera center derived from the projection matrix is\n",
    "${\\begin{bmatrix}-2.1526 & -3.3503 & 0.4023\\end{bmatrix}}^T$. After rescaling, the\n",
    "camera center becomes ${\\begin{bmatrix}-1.5120 & -2.3532 & 0.2826\\end{bmatrix}}^T$,\n",
    "which closely aligns with the ground truth value provided by the task. This confirms the\n",
    "robustness of the calibration and transformation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_m, res = generate_projection_matrix(norm_pts2d_left, norm_pts3d)\n",
    "tmp = get_camera_center(proj_m)\n",
    "print(f\"Camera Center without rescaling: {tmp}\")\n",
    "print(f\"Camera Center after rescaling is: \", 0.2826 * tmp / tmp[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering 2D Image Points and 3D World Points\n",
    "\n",
    "It's vital to recognize that not all matching 2D image points and 3D world points should\n",
    "be incorporated into the algorithm. Real-world applications frequently introduce\n",
    "inaccuracies in point matching. In practice, we can enhance accuracy by applying\n",
    "filtering techniques such as RANSAC to identify and exclude inaccurate matching points\n",
    "during the camera matrix estimation process.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Matrix Estimation\n",
    "\n",
    "In our pursuit to estimate the mapping of points in one image to lines in another using\n",
    "the fundamental matrix, we will apply methodologies akin to those employed in projection\n",
    "matrix estimation. Our approach will rely on the corresponding point coordinates found\n",
    "in `pts2d-pic_a.txt` and `pts2d-pic_b.txt`. Recall that the Fundamental Matrix is\n",
    "defined as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "u^{\\prime} & v^{\\prime} & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "f_{11} & f_{12} & f_{13} \\\\\n",
    "f_{21} & f_{22} & f_{23} \\\\\n",
    "f_{31} & f_{32} & f_{33} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "u \\\\\n",
    "v \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}\n",
    "= 0\n",
    "$$\n",
    "\n",
    "Again, we will follow algorithm 11.1 in MVG. The essential idea is very similar to\n",
    "estimating projection matrix.\n",
    "\n",
    "<img src=\"images/MVG-algo_11.1.png\" style=\"width:512px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_A(pts2d_left, pts2d_right):\n",
    "    num_points = pts2d_left.shape[1]\n",
    "    tmp = np.zeros((num_points, 9))\n",
    "    tmp[:, 0] = np.multiply(pts2d_left[0, :], pts2d_right[0, :])\n",
    "    tmp[:, 1] = np.multiply(pts2d_left[1, :], pts2d_right[0, :])\n",
    "    tmp[:, 2] = pts2d_right[0, :]\n",
    "    tmp[:, 3] = np.multiply(pts2d_left[0, :], pts2d_right[1, :])\n",
    "    tmp[:, 4] = np.multiply(pts2d_left[1, :], pts2d_right[1, :])\n",
    "    tmp[:, 5] = pts2d_right[1, :]\n",
    "    tmp[:, 6] = pts2d_left[0, :]\n",
    "    tmp[:, 7] = pts2d_left[1, :]\n",
    "    tmp[:, 8] = 1\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def constrain_fundamental_matrix(f):\n",
    "    u, s, v = np.linalg.svd(f, full_matrices=True)\n",
    "    D_head = np.array([[s[0], 0, 0], [0, s[1], 0], [0, 0, 0]])\n",
    "    return np.dot(np.dot(u, D_head), v)  # NOTE: v or v.T\n",
    "\n",
    "\n",
    "def denormalize_fundamental_matrix(T_prime, F, T):\n",
    "    return np.dot(np.dot(T_prime.T, F), T)\n",
    "\n",
    "\n",
    "def generate_fundamental_matrix(pts2d_left, pts2d_right):\n",
    "    A = compose_A(pts2d_left, pts2d_right)\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(np.dot(A.T, A))\n",
    "    min_arg = np.argmin(eigenvalues)\n",
    "    init_f = np.reshape(eigenvectors[:, min_arg], (3, 3))\n",
    "    F = constrain_fundamental_matrix(init_f)\n",
    "    return F\n",
    "\n",
    "\n",
    "norm_F = generate_fundamental_matrix(norm_pts2d_left, norm_pts2d_right)\n",
    "F = denormalize_fundamental_matrix(sim_mat_T_r, norm_F, sim_mat_T_l)\n",
    "\n",
    "epi_left = draw_epipolar_line_in_image(\n",
    "    left_points=pts2d_right, right_image=left_img, fundamental_matrix=F.T\n",
    ")\n",
    "epi_right = draw_epipolar_line_in_image(\n",
    "    left_points=pts2d_left, right_image=right_img, fundamental_matrix=F\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n",
    "ax[0].imshow(epi_left)\n",
    "ax[0].set_title(\"Epipolar lines in left image\", fontsize=12)\n",
    "ax[1].imshow(epi_right)\n",
    "ax[1].set_title(\"Epipolar lines in right image\", fontsize=12)\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].axis(\"off\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stereo Image Rectification\n",
    "\n",
    "In 3D computer vision, once the fundamental model is established, we can utilize the\n",
    "fundamental matrix for stereo image rectification. Image rectification involves\n",
    "resampling pairs of stereo images captured from significantly different viewpoints to\n",
    "produce a set of \"matched epipolar projections.\" These projections align the epipolar\n",
    "lines parallel to the x-axis, ensuring a consistent match between views. As a result,\n",
    "disparities between the images only exist in the x-direction, eliminating any\n",
    "y-disparity.\n",
    "\n",
    "As previously mentioned, we will adhere to the methodology detailed in MVG Section\n",
    "11.12, \"Image Rectification.\" Given matching image points\n",
    "$\\mathbf{x}_{i} \\leftrightarrow \\mathbf{x}'_{i}$ between the left and right images, our\n",
    "objective is to determine the projective transformations `H1` and `H2` to rectify the\n",
    "left and right images. The steps for this procedure are summarized as follows:\n",
    "\n",
    "1. Calculate the fundamental matrix and identify the epipoles `e1` and `e2` in the left\n",
    "   and right images.\n",
    "\n",
    "2. Select a projective transformation `H2` that maps the epipole `e2` to the point at\n",
    "   infinity, ${\\begin{bmatrix}1 & 0 & 0\\end{bmatrix}}^T$.\n",
    "\n",
    "3. Determine the optimal matching projective transformation `H1` that minimizes the\n",
    "   least-squares distance defined as:\n",
    "    $$\n",
    "    \\sum_i{d(H_{1}\\mathbf{x}_{i}, H_{2}\\mathbf{x}'_{i})^2}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Epipoles from the Fundamental Matrix\n",
    "\n",
    "Given the fundamental matrix $F$, epipoles are determined by the equations $Fe_{1} = 0$\n",
    "and $F^{T}e_{2} = 0$. To obtain the epipole, follow these steps:\n",
    "\n",
    "1. Perform Singular Value Decomposition on the fundamental matrix $F$ to acquire its SVD\n",
    "   decomposition: $F=U\\Sigma V^T$. Here, $U$ and $V$ are orthogonal matrices, and\n",
    "   $\\Sigma$ is a diagonal matrix containing singular values.\n",
    "\n",
    "2. Extract $e$ as the rightmost column of $V$ (i.e., the last column).\n",
    "\n",
    "3. Ensure that $e$ is in homogeneous coordinates, representing them as 3x1 column\n",
    "   vectors with the third component set to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epipole(fundamental_matrix):\n",
    "    \"\"\"Compute epipole using the fundamental matrix.\"\"\"\n",
    "    u, s, v = np.linalg.svd(fundamental_matrix)\n",
    "    e = v[-1, :]\n",
    "    e /= e[2]\n",
    "    return e  # (3, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Points at Infinity\n",
    "\n",
    "For an arbitrarily placed point of interest $\\mathbf{x}_0$ and epipole $e$, the required\n",
    "mapping $H$ is a composite transformation $H = GRT$. Here, $T$ represents a translation\n",
    "that relocates the point $\\mathbf{x}_0$ to the origin, $R$ signifies a rotation about\n",
    "the origin that aligns the epipole $e_{2}$ with a point\n",
    "${\\begin{bmatrix}f & 0 & 1\\end{bmatrix}}^T$ on the $x$-axis, and $G$ is the\n",
    "transformation that extends this point to infinity. This composite mapping is, to\n",
    "first-order, a rigid transformation in the vicinity of $\\mathbf{x}_0$. It's worth noting\n",
    "that when applying the translation $T$ to the entire image, we subsequently need to\n",
    "restore the image to its original coordinates, which is why `np.linalg.inv(T)` is\n",
    "employed at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_homography(image, epipole):\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Shift epipole's coord wrt the center of the image.\n",
    "    T = np.array([[1, 0, -width / 2], [0, 1, -height / 2], [0, 0, 1]])\n",
    "    e_centered = np.dot(T, epipole)\n",
    "    e_centered = e_centered / e_centered[2]\n",
    "\n",
    "    e_x = e_centered[0]\n",
    "    e_y = e_centered[1]\n",
    "    theta = np.arctan2(e_y, e_x)\n",
    "    R = np.array(\n",
    "        [\n",
    "            [np.cos(theta), -np.sin(theta), 0],\n",
    "            [np.sin(theta), np.cos(theta), 0],\n",
    "            [0, 0, 1],\n",
    "        ]\n",
    "    )  # Rotation matrix\n",
    "    # Take inverse because we are not rotating a point to aligh with `e_centered` but\n",
    "    # rotating `e_centered` to algin with x-axis.\n",
    "    R = np.linalg.inv(R)\n",
    "\n",
    "    # Now e_centered has the form: (f, 0, 1)\n",
    "    e_centered = np.dot(R, e_centered)\n",
    "    f = e_centered[0]\n",
    "    # Move the epipole to infinity\n",
    "    G = np.array([[1, 0, 0], [0, 1, 0], [-1 / f, 0, 1]])\n",
    "\n",
    "    # create the overall transformation matrix\n",
    "    H = np.dot(np.linalg.inv(T), np.dot(G, np.dot(R, T)))\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing the Least-Squares Distance\n",
    "\n",
    "Once `H2` is determined, we can derive the corresponding projective transformation `H1`\n",
    "that minimizes the expression:\n",
    "\n",
    "$$\n",
    "\\sum_i{d(H_{1}\\mathbf{x}_{i}, H_{2}\\mathbf{x}'_{i})^2}\n",
    "$$\n",
    "\n",
    "For a detailed algorithm, please refer to Section 11.12.2 and Corollary 11.4 in MVG.\n",
    "It's important to note that when calculating the 3x3 submatrix $M$ of the camera matrix,\n",
    "it must be made non-singular to prevent numerical instability. One common approach is to\n",
    "introduce noise to the target matrix, and the following matrix can be employed for this\n",
    "purpose (as discussed\n",
    "[here](https://dsp.stackexchange.com/questions/89480/understanding-and-resolving-singularities-in-stereo-rectification-using-multiple)):\n",
    "\n",
    "```Python\n",
    "noise = np.array([[e[0], e[0], e[0]],\n",
    "                  [e[1], e[1], e[1]],\n",
    "                  [e[2], e[2], e[2]]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_distance(e, F, H, pts2d_left, pts2d_right):\n",
    "    noise = np.array([[e[0], e[0], e[0]], [e[1], e[1], e[1]], [e[2], e[2], e[2]]])\n",
    "    skewed_e = np.array([[0, -e[2], e[1]], [e[2], 0, -e[0]], [-e[1], e[0], 0]])\n",
    "    M = np.dot(skewed_e, F) + noise  # Singular matrix to non-singular matrix\n",
    "    proj_pts2d_left = np.dot(np.dot(H, M), pts2d_left)  # 3 x num_pts\n",
    "    proj_pts2d_right = np.dot(H, pts2d_right)\n",
    "    proj_pts2d_left /= proj_pts2d_left[2, :]  # 3 x num_pts\n",
    "    proj_pts2d_right /= proj_pts2d_right[2, :]\n",
    "    x_coords = proj_pts2d_right[0, :]  # (num_pts,)\n",
    "    # np.linalg.lstsq((M, N) array_like, {(M,), (M, K)} array_like)\n",
    "    (a, b, c) = np.linalg.lstsq(proj_pts2d_left.T, x_coords, rcond=None)[0]\n",
    "    H_A = np.array([[a, b, c], [0, 1, 0], [0, 0, 1]])\n",
    "    H1 = np.dot(H_A, np.dot(H, M))\n",
    "    return H1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together\n",
    "\n",
    "With all the necessary components in place, we can now compute the projective\n",
    "transformations `H1` and `H2`. These transformations will be applied to rectify and warp\n",
    "the original left and right images. Additionally, we will project our original 2D image\n",
    "points, `pts2d_left` and `pts2d_right`, onto the rectified 2D coordinate space,\n",
    "resulting in `proj_pts2d_left` and `proj_pts2d_right`. Subsequently, we will follow the\n",
    "previously outlined procedure to recompute the fundamental matrix using\n",
    "`proj_pts2d_left` and `proj_pts2d_right`. This allows us to generate new epipolar lines\n",
    "within the rectified images.\n",
    "\n",
    "Furthermore, an intriguing aspect to note is the differing use of the transformation\n",
    "matrix. When projecting 2D image points with `H1`, it employs the direct matrix, whereas\n",
    "during image warping, it utilizes the **inverse** of the matrix to avoid \"holes\" in the\n",
    "interpolated rectified image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_matching_projective_transformation(\n",
    "    f_mat, right_img, pts2d_left, pts2d_right\n",
    "):\n",
    "    e2 = compute_epipole(f_mat.T)\n",
    "    H2 = select_homography(right_img, e2)\n",
    "    H1 = minimize_distance(e2, f_mat, H2, pts2d_left, pts2d_right)\n",
    "    return H1, H2\n",
    "\n",
    "\n",
    "H1, H2 = compute_matching_projective_transformation(\n",
    "    F, right_img, pts2d_left, pts2d_right\n",
    ")\n",
    "rectified_img_left = warp(left_img, ProjectiveTransform(matrix=np.linalg.inv(H1)))\n",
    "rectified_img_right = warp(right_img, ProjectiveTransform(matrix=np.linalg.inv(H2)))\n",
    "\n",
    "proj_pts2d_left = np.dot(H1, pts2d_left)\n",
    "proj_pts2d_left = proj_pts2d_left / proj_pts2d_left[2, :]\n",
    "proj_pts2d_right = np.dot(H2, pts2d_right)\n",
    "proj_pts2d_right = proj_pts2d_right / proj_pts2d_right[2, :]\n",
    "\n",
    "norm_pts2d_left, sim_mat_T_l = normalize_image_points(\n",
    "    proj_pts2d_left, avg_distance=np.sqrt(2)\n",
    ")\n",
    "norm_pts2d_right, sim_mat_T_r = normalize_image_points(\n",
    "    proj_pts2d_right, avg_distance=np.sqrt(2)\n",
    ")\n",
    "\n",
    "new_F = denormalize_fundamental_matrix(\n",
    "    sim_mat_T_r,\n",
    "    generate_fundamental_matrix(norm_pts2d_left, norm_pts2d_right),\n",
    "    sim_mat_T_l,\n",
    ")\n",
    "\n",
    "rectified_img_left *= 255\n",
    "rectified_img_left = rectified_img_left.astype(np.uint8) \n",
    "rectified_img_right *= 255\n",
    "rectified_img_right = rectified_img_right.astype(np.uint8) \n",
    "epi_right = draw_epipolar_line_in_image(\n",
    "    left_points=proj_pts2d_left, right_image=rectified_img_right, fundamental_matrix=new_F\n",
    ")\n",
    "epi_left = draw_epipolar_line_in_image(\n",
    "    left_points=proj_pts2d_right,\n",
    "    right_image=rectified_img_left,\n",
    "    fundamental_matrix=new_F.T,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n",
    "ax[0].imshow(epi_left)\n",
    "ax[0].set_title(\"Rectified left image\", fontsize=12)\n",
    "ax[1].imshow(epi_right)\n",
    "ax[1].set_title(\"Rectified right image\", fontsize=12)\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].axis(\"off\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Alternative Approach for Transformation Matrix Calculation\n",
    "\n",
    "In this tutorial, we follow algorithms discussed in \"Multiple View Geometry in Computer\n",
    "Vision (MVG)\" for various stereo geometry calculations. However, it is clear that the\n",
    "initial rectified image suffers from significant distortion. To address this issue, an\n",
    "alternative method for rectifying stereo images with reduced distortion is explored in\n",
    "the paper \"Computing Rectifying Homographies for Stereo Vision\" by Charles Loop and\n",
    "Zhengyou Zhang.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_Introduction_to_Computer_Vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
