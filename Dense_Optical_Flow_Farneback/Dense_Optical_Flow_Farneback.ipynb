{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Optical Flow with Gunnar Farnebäck's Algorithm\n",
    "\n",
    "Dense optical flow identifies the movement of each pixel across a series of images. In\n",
    "this notebook, I'll build Farnebäck's algorithm from the ground up.\n",
    "\n",
    "**RULES:** As usual, **`OpenCV`** is banned in this repository.\n",
    "\n",
    "**References:**\n",
    "\n",
    "-   [1]\n",
    "    [Polynomial Expansion for Orientation and Motion Estimation](https://www.ida.liu.se/ext/WITAS-ev/Computer_Vision_Technologies/PaperInfo/farneback02.html)\n",
    "\n",
    "-   [2]\n",
    "    [Two-Frame Motion Estimation Based on Polynomial Expansion](https://link.springer.com/chapter/10.1007/3-540-45103-X_50)\n",
    "\n",
    "-   [3]\n",
    "    [Optical Flow - Michael Black - MLSS 2013 Tübingen](https://www.youtube.com/watch?v=tIwpDuqJqcE)\n",
    "\n",
    "-   [4]\n",
    "    [ericPrince's Pure python implementation of Gunnar Farneback's optical flow algorithm](https://github.com/ericPrince/optical-flow)\n",
    "\n",
    "**Important Note:** Although [2] is the most widely cited paper on Farnebäck's\n",
    "algorithm, a thorough comprehension of the algorithm requires reading Gunnar Farnebäck's\n",
    "complete thesis [1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import correlate1d\n",
    "from functools import partial\n",
    "import skimage\n",
    "import my_utils\n",
    "\n",
    "# np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "In the world of computer vision, tracking the displacement of pixels across consecutive\n",
    "frames is usually based on two assumptions:\n",
    "\n",
    "1. **Brightness Constancy:** It states that the brightness of a pixel remains constant\n",
    "   between consecutive images, despite its movement to a new position. This idea is\n",
    "   captured by the equation:\n",
    "\n",
    "    $$\n",
    "    I(x + u, y + v, t + 1) = I(x, y, t)\n",
    "    $$\n",
    "\n",
    "    Here, $u$ and $v$ represent the horizontal and vertical shifts in the pixel's\n",
    "    position, illustrating that the pixel's intensity doesn't change as it moves.\n",
    "\n",
    "2. **Spatial Smoothness:** It is based on the idea that neighboring pixels usually move\n",
    "   in a similar fashion because they are likely part of the same surface. Surfaces tend\n",
    "   to be smooth, implying that adjacent pixels will have comparable motion. This concept\n",
    "   is summarized as follows:\n",
    "\n",
    "    $$\n",
    "    u_p = u_n \\quad \\text{and} \\quad v_p = v_n, \\quad \\forall n \\in G(p)\n",
    "    $$\n",
    "\n",
    "    It means that the movement of a pixel $p$ in both horizontal ($u$) and vertical\n",
    "    ($v$) directions is similar to that of its neighboring pixel $n$, suggesting that\n",
    "    optical flow changes smoothly across the image.\n",
    "\n",
    "**Objective Function:**\n",
    "\n",
    "Derived from these assumptions, we can define the objective functions as follow:\n",
    "\n",
    "-   **Brightness Constancy:**\n",
    "\n",
    "    $$\n",
    "    E_D(u, v) = \\sum (I(x + u, y + v, t + 1) - I(x, y, t))^2\n",
    "    $$\n",
    "\n",
    "    This equation emphasizes that deviations from brightness constancy are minimized,\n",
    "    assuming the presence of Gaussian noise.\n",
    "\n",
    "-   **Spatial Smoothness:**\n",
    "\n",
    "    $$\n",
    "    E_s(u, v) = \\sum(u_p - u_n)^2 + \\sum(v_p - v_n)^2, \\quad \\forall n \\in G(p)\n",
    "    $$\n",
    "\n",
    "    This formula encourages smoothness by penalizing variations in the motion between a\n",
    "    pixel and its neighbors.\n",
    "\n",
    "**Solving the Equations**\n",
    "\n",
    "The principle of brightness constancy implies that a pixel's intensity does not change\n",
    "over time as it moves. This is mathematically represented as:\n",
    "\n",
    "$$\n",
    "I(x, y, t) = I(x + \\Delta x, y + \\Delta y, t + \\Delta t)\n",
    "$$\n",
    "\n",
    "For slight movements ($\\Delta x$, $\\Delta y$, $\\Delta t$), we can use the first-order\n",
    "Taylor series expansion for image intensity $I$:\n",
    "\n",
    "$$\n",
    "I(x + \\Delta x, y + \\Delta y, t + \\Delta t) \\approx I(x, y, t) + \\frac{\\partial I}{\\partial x} \\Delta x + \\frac{\\partial I}{\\partial y} \\Delta y + \\frac{\\partial I}{\\partial t} \\Delta t\n",
    "$$\n",
    "\n",
    "By applying the brightness constancy condition and simplifying, we eliminate the term\n",
    "$I(x, y, t)$ on both sides:\n",
    "\n",
    "$$\n",
    "I_x \\Delta x + I_y \\Delta y + I_t \\Delta t = 0\n",
    "$$\n",
    "\n",
    "Dividing through by $\\Delta t$ (assuming it is not zero) and using\n",
    "$\\Delta x / \\Delta t = u$ and $\\Delta y / \\Delta t = v$ for velocity components, we\n",
    "arrive at the optical flow constraint equation:\n",
    "\n",
    "$$\n",
    "I_x u + I_y v + I_t = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Expansion for Orientation and Motion Estimation\n",
    "\n",
    "Polynomial expansion fits polynomials to local pixel neighborhoods to approximate image\n",
    "motion and structure as detailed in [1]. It uses quadratic, linear, and constant terms\n",
    "(A, B, C) to describe local image structure and motion:\n",
    "\n",
    "-   **Quadratic Term (A):** Represents the curvature of the image intensity surface,\n",
    "    indicating the geometric structure like edges or flat areas. High values signal\n",
    "    significant curvature or rapid intensity changes.\n",
    "\n",
    "-   **Linear Term (B):** Represents the gradient at each pixel, showing the direction\n",
    "    and magnitude of the most substantial intensity change, crucial for identifying\n",
    "    motion direction and feature orientation.\n",
    "\n",
    "-   **Constant Term (C):** Represents the average intensity in a pixel's neighborhood,\n",
    "    indicating the area's overall brightness or darkness without detailing structure or\n",
    "    motion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal, Certainty, and Applicability\n",
    "\n",
    "The signal $f$ and its local approximation $\\hat{f}$, a finite-dimensional vector in\n",
    "$C^n$, reflect the signal within a specific neighborhood, represented as an $n \\times 1$\n",
    "column vector.\n",
    "\n",
    "**Certainty** is the confidence in signal values, indicated by non-negative real\n",
    "numbers. The field of certainty is $c$, with $\\hat{c}$ as the $n \\times 1$ vector for\n",
    "local certainty levels.\n",
    "\n",
    "**Applicability** defines the relevance of basis functions within the neighborhood,\n",
    "expressed as non-negative values in an $n \\times 1$ vector, $a$. Unlike certainty,\n",
    "applicability focuses on the significance of each point, with non-zero values indicating\n",
    "relevance. While intuitively applicability might range between $[0, 1]$, no such\n",
    "restriction is necessary, as the scale of these values doesn't impact their relevance.\n",
    "\n",
    "For generating applicability and certainty specifics, see Section 3.10, Section 4.2 and\n",
    "Section 4.3 of [1]. The functions `generate_applicability` and `generate_certainty`\n",
    "detail their implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_applicability(sigma):\n",
    "    \"\"\"Calculate 1D Gaussian applicability kernel.\"\"\"\n",
    "    n = int(4 * sigma + 1)  # Capture significant parts of the Gaussian distribution\n",
    "    x = np.arange(-n, n + 1)\n",
    "    applicability = np.exp(-(x**2) / (2 * sigma**2))\n",
    "    return x, applicability\n",
    "\n",
    "\n",
    "def generate_certainty(height, width, denominator=5):\n",
    "    \"\"\"should it be gaussion or linear\"\"\"\n",
    "    kernel = np.minimum(\n",
    "        1,\n",
    "        1 / denominator * np.minimum(np.arange(height)[:, None], np.arange(width)),\n",
    "    )\n",
    "    kernel = np.minimum(\n",
    "        kernel,\n",
    "        1\n",
    "        / denominator\n",
    "        * np.minimum(\n",
    "            height - 1 - np.arange(height)[:, None],\n",
    "            width - 1 - np.arange(width),\n",
    "        ),\n",
    "    )\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separable Correlation\n",
    "\n",
    "Separable correlation is a technique that enhances computational efficiency in signal\n",
    "processing and image analysis by simplifying correlation operations, especially with\n",
    "large filters. It involves decomposing a two-dimensional kernel into two orthogonal\n",
    "one-dimensional kernels, allowing two-dimensional correlation to be performed as two\n",
    "separate one-dimensional processes: first across rows, then down columns, or vice versa.\n",
    "\n",
    "Mathematically, if a two-dimensional kernel $H$ can be expressed as the outer product of\n",
    "two one-dimensional vectors $u$ and $v$:\n",
    "\n",
    "$$\n",
    "H = u \\otimes v\n",
    "$$\n",
    "\n",
    "then the kernel is separable. For an input signal or image $I$, the two-dimensional\n",
    "correlation with a separable kernel $H$ occurs in two phases:\n",
    "\n",
    "1. **Horizontal Pass:** Apply one-dimensional kernel $u$ across each row of $I$.\n",
    "2. **Vertical Pass:** Apply one-dimensional kernel $v$ down each column of the\n",
    "   horizontal pass result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining `poly_exp`\n",
    "\n",
    "To construct the polynomial expansion function `poly_exp`, we follow the guidelines of\n",
    "Chapter 4 in [1]. The process is divided into four main parts:\n",
    "\n",
    "-   **Initialization and Applicability Generation:** Detailed on Section 4.3, this step\n",
    "    involves setting up the initial conditions and generating the applicability matrix\n",
    "    based on $\\sigma$.\n",
    "\n",
    "-   **Calculation of Polynomial Coefficients `b`:** As described in Section 4.3, this\n",
    "    involves calculating the polynomial expansion coefficients for horizontal\n",
    "    (x-direction) and vertical (y-direction) correlations separately. This step also\n",
    "    includes multiplying the certainty map with the image signal to prioritize signal\n",
    "    values based on their certainty.\n",
    "\n",
    "-   **Cross-Correlation and Polynomial Parameters Calculation:** Utilizes matrices `G`\n",
    "    and `v` to solve for `r` using Equation (4.9), where $r = G^{-1}v$. Here, `r`\n",
    "    represents the parameters of the second-order polynomial modeling the signal `f`.\n",
    "\n",
    "-   **Final Polynomial Terms Extraction:** Based on Equation (4.3), the quadratic (`A`),\n",
    "    linear (`B`), and constant (`C`) terms of the polynomial are derived, respectively.\n",
    "\n",
    "Finally, the dimensions of the coefficients and residual error calculation are\n",
    "summarized as follows:\n",
    "\n",
    "-   Coefficient `b`: (n, n, 6)\n",
    "-   Parameters `r`: (f, f, 6)\n",
    "-   Signal `f`: (f, f)\n",
    "-   Residual error $e$ is calculated as $\\|b \\cdot r - f\\|_W$, as shown in equation\n",
    "    3.19.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_exp(f, certainty, sigma):\n",
    "    # Initialization and Applicability Generation\n",
    "    height, width = f.shape\n",
    "    x, applicability = generate_applicability(sigma)\n",
    "\n",
    "    # Calculation of Polynomial Coefficients `b`\n",
    "    bx = np.stack(\n",
    "        [\n",
    "            np.ones(applicability.shape),\n",
    "            x,\n",
    "            np.ones(applicability.shape),\n",
    "            x**2,\n",
    "            np.ones(applicability.shape),\n",
    "            x,\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )  # (n, 6)\n",
    "    by = np.stack(\n",
    "        [\n",
    "            np.ones(applicability.shape),\n",
    "            np.ones(applicability.shape),\n",
    "            x,\n",
    "            np.ones(applicability.shape),\n",
    "            x**2,\n",
    "            x,\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )  # (n, 6)\n",
    "\n",
    "    # Pre-calculate product of certainty and signal\n",
    "    cf = certainty * f\n",
    "\n",
    "    # Cross-Correlation and Polynomial Parameters Calculation\n",
    "    G = np.empty(list(f.shape) + [bx.shape[-1]] * 2)  # (height, width, 6, 6)\n",
    "    v = np.empty(list(f.shape) + [bx.shape[-1]])  # (height, width, 6)\n",
    "\n",
    "    # Apply separable cross-correlations\n",
    "    # Pre-calculate quantities.\n",
    "    ab = np.einsum(\"i,ij->ij\", applicability, bx)\n",
    "    abb = np.einsum(\"ij,ik->ijk\", ab, bx)\n",
    "    # Calculate G and v for each pixel with cross-correlation\n",
    "    for i in range(bx.shape[-1]):\n",
    "        for j in range(bx.shape[-1]):\n",
    "            G[..., i, j] = correlate1d(\n",
    "                certainty, abb[..., i, j], axis=0, mode=\"constant\", cval=0\n",
    "            )\n",
    "\n",
    "        v[..., i] = correlate1d(cf, ab[..., i], axis=0, mode=\"constant\", cval=0)\n",
    "\n",
    "    # Pre-calculate quantities.\n",
    "    ab = np.einsum(\"i,ij->ij\", applicability, by)\n",
    "    abb = np.einsum(\"ij,ik->ijk\", ab, by)\n",
    "    # Calculate G and v for each pixel with cross-correlation\n",
    "    for i in range(bx.shape[-1]):\n",
    "        for j in range(bx.shape[-1]):\n",
    "            G[..., i, j] = correlate1d(\n",
    "                G[..., i, j], abb[..., i, j], axis=1, mode=\"constant\", cval=0\n",
    "            )\n",
    "\n",
    "        v[..., i] = correlate1d(v[..., i], ab[..., i], axis=1, mode=\"constant\", cval=0)\n",
    "\n",
    "    # Solve r for each pixel\n",
    "    r = np.linalg.solve(G, v)\n",
    "\n",
    "    # Final Polynomial Terms Extraction\n",
    "    # Quadratic term\n",
    "    A = np.empty(list(f.shape) + [2, 2])\n",
    "    A[..., 0, 0] = r[..., 3]\n",
    "    A[..., 0, 1] = r[..., 5] / 2\n",
    "    A[..., 1, 0] = A[..., 0, 1]\n",
    "    A[..., 1, 1] = r[..., 4]\n",
    "\n",
    "    # Linear term\n",
    "    B = np.empty(list(f.shape) + [2])\n",
    "    B[..., 0] = r[..., 1]\n",
    "    B[..., 1] = r[..., 2]\n",
    "\n",
    "    # constant term\n",
    "    C = r[..., 0]\n",
    "\n",
    "    return A, B, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the polynomial expansion function `poly_exp`, we can visualize the terms\n",
    "`A`, `B`, and `C` as follows. The quadratic term `A` captures the even part of the\n",
    "signal, while the linear term `B` encapsulates the odd part. Meanwhile, `C` reflects\n",
    "variations in the local differential convolution level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"input/yosemite/yos02.tif\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "print(\"img:\", img.shape, img.dtype, img.min(), img.max())\n",
    "# TODO: Use float64 should also work. Input image should be floating point.\n",
    "# gray = gray.astype(np.float64)\n",
    "# gray /= 255\n",
    "print(f\"gray: {gray.shape}, {gray.dtype}, {gray.min()}, {gray.max()}\")\n",
    "\n",
    "height, width, *_ = img.shape\n",
    "certainty = generate_certainty(height, width, denominator=5)\n",
    "print(f\"certainty: {certainty.shape}, {certainty.dtype}, {certainty.min()}, {certainty.max()}\")\n",
    "\n",
    "A, B, C = poly_exp(f=gray, certainty=certainty, sigma=4)\n",
    "print(\"A:\", A.shape)\n",
    "print(\"B:\", B.shape)\n",
    "print(\"C:\", C.shape)\n",
    "\n",
    "my_utils.visualize_polynomial_expansion(\n",
    "    img, A, B, C, out_path=\"./output/polynomial_expansion.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displacement Estimation\n",
    "\n",
    "Polynomial expansion allows for approximating the neighborhood of a pixel. When\n",
    "considering an ideal translation, we analyze the behavior of a quadratic polynomial\n",
    "signal:\n",
    "\n",
    "$$\n",
    "f_1(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{A}_1 \\mathbf{x} + \\mathbf{b}_1^T \\mathbf{x} + c_1\n",
    "$$\n",
    "\n",
    "and construct a new signal $f_2$ displaced globally by $\\mathbf{d}$, as shown in\n",
    "Equation (7.2) in [1]:\n",
    "\n",
    "$$\n",
    "f_2(\\mathbf{x}) = f_1(\\mathbf{x} - \\mathbf{d}) = \\mathbf{x}^T \\mathbf{A}_2 \\mathbf{x} + \\mathbf{b}_2^T \\mathbf{x} + c_2.\n",
    "$$\n",
    "\n",
    "By equating coefficients in the quadratic polynomials, we find:\n",
    "\n",
    "-   $\\mathbf{A}_2 = \\mathbf{A}_1$,\n",
    "-   $\\mathbf{b}_2 = \\mathbf{b}_1 - 2\\mathbf{A}_1\\mathbf{d}$,\n",
    "-   $c_2 = \\mathbf{d}^T \\mathbf{A}_1 \\mathbf{d} - \\mathbf{b}_1^T \\mathbf{d} + c_1$.\n",
    "\n",
    "For non-singular $\\mathbf{A}_1$, the translation $\\mathbf{d}$ can be calculated as:\n",
    "\n",
    "$$\n",
    "\\mathbf{d} = -\\frac{1}{2} \\mathbf{A}_1^{-1} (\\mathbf{b}_2 - \\mathbf{b}_1).\n",
    "$$\n",
    "\n",
    "This principle, as [1] states, is applicable regardless of signal dimensionality.\n",
    "\n",
    "Furthermore, [1] suggests utilizing local polynomial approximations for each pixel\n",
    "across images, implying the use of $\\mathbf{A}_1(\\mathbf{x})$,\n",
    "$\\mathbf{b}_1(\\mathbf{x})$, and $c_1(\\mathbf{x})$ for the first image, and\n",
    "$\\mathbf{A}_2(\\mathbf{x})$, $\\mathbf{b}_2(\\mathbf{x})$, and $c_2(\\mathbf{x})$ for the\n",
    "second. Ideally, this would result in $\\mathbf{A}_1 = \\mathbf{A}_2$, but practically, an\n",
    "approximation is used:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}(\\mathbf{x}) = \\frac{\\mathbf{A}_1(\\mathbf{x}) + \\mathbf{A}_2(\\mathbf{x})}{2}.\n",
    "$$\n",
    "\n",
    "Additionally, [1] defines\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{b}(\\mathbf{x}) = -\\frac{1}{2}(\\mathbf{b}_2(\\mathbf{x}) - \\mathbf{b}_1(\\mathbf{x})),\n",
    "$$\n",
    "\n",
    "leading to the primary constraint for displacement estimation:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}(\\mathbf{x})\\mathbf{d}(\\mathbf{x}) = \\Delta \\mathbf{b}(\\mathbf{x}),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{d}(\\mathbf{x})$ represents a spatially varying displacement field, moving\n",
    "from a global to a local displacement context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Displacement over a Neighborhood\n",
    "\n",
    "Section 7.3 in [1] indicates that solving the equation\n",
    "\n",
    "$$\n",
    "\\mathbf{A}(\\mathbf{x})\\mathbf{d}(\\mathbf{x}) = \\Delta \\mathbf{b}(\\mathbf{x}),\n",
    "$$\n",
    "\n",
    "pointwise does not yield satisfactory results. To improve accuracy, [1] recommends\n",
    "optimizing the solution across a neighborhood $I$ around $\\mathbf{x}$. This is achieved\n",
    "by minimizing the weighted sum:\n",
    "\n",
    "$$\n",
    "\\sum_{\\Delta \\mathbf{x} \\in I} w(\\Delta \\mathbf{x})\\|\\mathbf{A}(\\mathbf{x} + \\Delta \\mathbf{x})\\mathbf{d}(\\mathbf{x}) - \\Delta \\mathbf{b}(\\mathbf{x} + \\Delta \\mathbf{x})\\|^2,\n",
    "$$\n",
    "\n",
    "where $w(\\Delta \\mathbf{x})$ serves as a weight function, enhancing the contribution of\n",
    "more relevant points within the neighborhood. The optimal displacement\n",
    "$\\mathbf{d}(\\mathbf{x})$ is found with:\n",
    "\n",
    "$$\n",
    "\\mathbf{d}(\\mathbf{x}) = \\left(\\sum w\\mathbf{A}^T \\mathbf{A}\\right)^{-1} \\sum w\\mathbf{A}^T \\Delta \\mathbf{b},\n",
    "$$\n",
    "\n",
    "after simplifying the notation for clarity. The corresponding minimum error\n",
    "$e(\\mathbf{x})$, indicating the solution's accuracy, is expressed as:\n",
    "\n",
    "$$\n",
    "e(\\mathbf{x}) = (\\sum w\\Delta \\mathbf{b}^T \\Delta \\mathbf{b}) - \\mathbf{d}(\\mathbf{x})^T \\left(\\sum w\\mathbf{A}^T \\Delta \\mathbf{b}\\right).\n",
    "$$\n",
    "\n",
    "This formulation computes $\\mathbf{A}^T \\mathbf{A}$, $\\mathbf{A}^T \\Delta \\mathbf{b}$,\n",
    "and $\\Delta \\mathbf{b}^T \\Delta \\mathbf{b}$ pointwise, then averages these with the\n",
    "weight $w$ before calculating the displacement. The error $e(\\mathbf{x})$ inversely\n",
    "relates to confidence: smaller values indicate greater reliability. The solution is both\n",
    "existent and unique unless the neighborhood is uniformly affected by the aperture\n",
    "problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating a Parameterized Displacement Field\n",
    "\n",
    "Section 6.3.1 in [1] shows that the simplest possible motion model:\n",
    "\n",
    "$$\n",
    "v_x(x, y) = a,\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_y(x, y) = b,\n",
    "$$\n",
    "\n",
    "where $x$ and $y$ are the spatial coordinates, $v_x$ and $v_y$ are the $x$ and $y$\n",
    "components of the velocity, and $a$ and $b$ are the model parameters. Geometrically,\n",
    "this motion model assumes that the velocity is constant over the region and corresponds\n",
    "to objects undergoing a pure translation under orthographic projection.\n",
    "\n",
    "Equation (6.5) in [1] shows a more powerful alternative, the affine motion model:\n",
    "\n",
    "$$\n",
    "v_x(x, y) = ax + by + c,\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_y(x, y) = dx + ey + f,\n",
    "$$\n",
    "\n",
    "which applies to planar patches undergoing rigid body motion, i.e., translation plus\n",
    "rotation, under orthographic projection. To also account for perspective projection, [1]\n",
    "introduces the eight-parameter motion model in Equation (6.6):\n",
    "\n",
    "$$\n",
    "v_x(x, y) = a_1 + a_2 x + a_3 y + a_7 x^2 + a_8 xy,\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_y(x, y) = a_4 + a_5 x + a_6 y + a_7 xy + a_8 y^2.\n",
    "$$\n",
    "\n",
    "The usefulness of these models depends on the application, but it is useful to note\n",
    "that, sufficiently far away, most surfaces can be approximated as planar. Moreover, if\n",
    "the distance to the scene is much larger than the variation in distance within the\n",
    "scene, perspective projection can be approximated by orthographic projection.\n",
    "\n",
    "Following the motion model introduced above, we can enhance the robustness of\n",
    "displacement estimation by parameterizing the displacement field according to the\n",
    "eight-parameter motion model.\n",
    "\n",
    "Section 7.5 in [1] rewrites the above formula to:\n",
    "\n",
    "$$\n",
    "d = Sp,\n",
    "$$\n",
    "\n",
    "where $S$ is the matrix containing the spatial variables,\n",
    "\n",
    "$$\n",
    "S = \\left[ \\begin{array}{cc}\n",
    "1 & x & y & 0 & 0 & 0 & x^2 & xy \\\\\n",
    "0 & 0 & 0 & 1 & x & y & xy & y^2 \\\\\n",
    "\\end{array} \\right],\n",
    "$$\n",
    "\n",
    "and $p$ is the parameter vector,\n",
    "\n",
    "$$\n",
    "p = \\left[ \\begin{array}{c}\n",
    "a_1 \\\\\n",
    "a_2 \\\\\n",
    "a_3 \\\\\n",
    "a_4 \\\\\n",
    "a_5 \\\\\n",
    "a_6 \\\\\n",
    "a_7 \\\\\n",
    "a_8 \\\\\n",
    "\\end{array} \\right].\n",
    "$$\n",
    "\n",
    "For the weighted least squares problem, we solve\n",
    "\n",
    "$$\n",
    "\\sum w_i \\|A_i S_i p - \\Delta b_i\\|^2,\n",
    "$$\n",
    "\n",
    "where $i$ indexes coordinates in a neighborhood. The solution is\n",
    "\n",
    "$$\n",
    "p = \\left( \\sum w_i S_i^T A_i^T A_i S_i \\right)^{-1} \\sum w_i S_i^T A_i^T \\Delta b_i,\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motion_model(x, model):\n",
    "    # Evaluate warp parametrization model at pixel coordinates\n",
    "    if model == \"constant\":\n",
    "        S = np.eye(2)\n",
    "\n",
    "    elif model in (\"affine\", \"eight_param\"):\n",
    "        # TODO: It seems affine and eight_param cannot produce valid results.\n",
    "        # (height, width, 6 or 8)\n",
    "        S = np.empty(list(x.shape) + [6 if model == \"affine\" else 8])\n",
    "\n",
    "        S[..., 0, 0] = 1\n",
    "        S[..., 0, 1] = x[..., 0]\n",
    "        S[..., 0, 2] = x[..., 1]\n",
    "        S[..., 0, 3] = 0\n",
    "        S[..., 0, 4] = 0\n",
    "        S[..., 0, 5] = 0\n",
    "\n",
    "        S[..., 1, 0] = 0\n",
    "        S[..., 1, 1] = 0\n",
    "        S[..., 1, 2] = 0\n",
    "        S[..., 1, 3] = 1\n",
    "        S[..., 1, 4] = x[..., 0]\n",
    "        S[..., 1, 5] = x[..., 1]\n",
    "\n",
    "        if model == \"eight_param\":\n",
    "            S[..., 0, 6] = x[..., 0] ** 2\n",
    "            S[..., 0, 7] = x[..., 0] * x[..., 1]\n",
    "\n",
    "            S[..., 1, 6] = x[..., 0] * x[..., 1]\n",
    "            S[..., 1, 7] = x[..., 1] ** 2\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid parametrization model\")\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refining the Model with Prior Knowledge\n",
    "\n",
    "One of the key challenges in estimating dense optical flow using polynomial expansions,\n",
    "as proposed by Gunnar Farnebäck, is the underlying assumption that the local polynomial\n",
    "approximations remain constant across two consecutive frames, differing only by a\n",
    "displacement vector. However, since these polynomial models are local approximations,\n",
    "their parameters can vary significantly across the image, leading to inaccuracies in the\n",
    "constraint equation:\n",
    "\n",
    "$$\n",
    "A(\\mathbf{x})\\mathbf{d}(\\mathbf{x}) = \\Delta b(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "This approximation holds relatively well for small displacements but becomes less\n",
    "reliable for larger movements. To mitigate this, the algorithm does not restrict the\n",
    "comparison of polynomials to identical coordinates in consecutive frames. With some\n",
    "prior knowledge of the motion, it's possible to compare the polynomial at a point\n",
    "$\\mathbf{x}$ in the first frame to that at a point\n",
    "$\\mathbf{x} + \\tilde{\\mathbf{d}}(\\mathbf{x})$ in the second frame, where\n",
    "$\\tilde{\\mathbf{d}}(\\mathbf{x})$ represents an initial, possibly rough, estimate of the\n",
    "displacement, quantized to integer pixel values. This technique aims to refine the\n",
    "estimate of the displacement vector by considering the difference between the real\n",
    "displacement and this initial estimate, which should be smaller and more manageable.\n",
    "\n",
    "This refinement is mathematically represented by modifying the equations to:\n",
    "\n",
    "$$\n",
    "A(\\mathbf{x}) = \\frac{A_1(\\mathbf{x}) + A_2(\\mathbf{x} + \\tilde{\\mathbf{d}}(\\mathbf{x}))}{2},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta b(\\mathbf{x}) = -\\frac{1}{2}(b_2(\\mathbf{x} + \\tilde{\\mathbf{d}}(\\mathbf{x})) - b_1(\\mathbf{x})) + A(\\mathbf{x}) \\tilde{\\mathbf{d}}(\\mathbf{x}),\n",
    "$$\n",
    "\n",
    "where $\\tilde{\\mathbf{d}}(\\mathbf{x})$ is the pre-estimated displacement vector. The\n",
    "term $A(\\mathbf{x}) \\tilde{\\mathbf{d}}(\\mathbf{x})$ reintegrates the quantized prior\n",
    "knowledge of displacement into the model. In the absence of any initial displacement\n",
    "($\\tilde{\\mathbf{d}}(\\mathbf{x}) = 0$), these equations simplify to their original\n",
    "forms, as expected.\n",
    "\n",
    "The algorithm, summarized in the last sections, is visualized in the following block\n",
    "diagram. The inputs are the coefficients of the quadratic polynomial expansions ($A_1$,\n",
    "$b_1$, $A_2$, $b_2$) of two consecutive frames and an initial displacement field\n",
    "$\\mathbf{d}_{in}$. The output is the refined displacement field $\\mathbf{d}_{out}$.\n",
    "\n",
    "<figure>\n",
    "  <div style=\"display: flex; justify-content: space-between; max-width: 300px; /* Adjust this value as needed */\">\n",
    "    <div style=\"text-align: center;\">\n",
    "      <img src=\"./images/fig_7_8.png\" style=\"max-width: 100%; height: auto;\">\n",
    "      <p><strong></strong></p>\n",
    "      <p>Block diagram of the basic displacement estimation algorithm (DE) adapted from [1].</p>\n",
    "    </div>\n",
    "  </div>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Reduction in Displacement Fields\n",
    "\n",
    "A detailed analysis of the residual displacement field reveals that the predominant\n",
    "sources of noise are regions lacking distinct structural features or with low contrast.\n",
    "Section 7.10.3 in [1] mitigates this problem by \"forcing\" the background displacement\n",
    "field onto these uncertain estimates. This is accomplished by introducing a\n",
    "regularization term, i.e., minimizing the expression:\n",
    "\n",
    "$$\n",
    "\\mu\\|\\mathbf{d}(\\mathbf{x}) - \\mathbf{d}_0 (\\mathbf{x})\\|^2 + \\sum_{\\Delta x \\in I} w(\\Delta x)\\|\\mathbf{A}(\\mathbf{x} + \\Delta \\mathbf{x})\\mathbf{d}(\\mathbf{x}) - \\Delta \\mathbf{b}(\\mathbf{x} + \\Delta \\mathbf{x})\\|^2,\n",
    "$$\n",
    "\n",
    "where $\\mathbf{d}_0$ represents the previously estimated background displacement field,\n",
    "and $\\mu$ is a weighting constant.\n",
    "\n",
    "The solution to the equation above is given by Equation (7.36) in [1] as:\n",
    "\n",
    "$$\n",
    "\\mathbf{d}(\\mathbf{x}) = \\left(\\mu \\mathbf{I} + \\sum w\\mathbf{A}^T \\mathbf{A}\\right)^{-1} \\left(\\mu \\mathbf{d}_0(\\mathbf{x}) + \\sum w\\mathbf{A}^T \\Delta \\mathbf{b}\\right),\n",
    "$$\n",
    "\n",
    "The corresponding block diagram, illustrating the adaptation of the basic displacement\n",
    "estimation algorithm to include this noise reduction technique, is designated below.\n",
    "\n",
    "[1] suggests determining $\\mu$ by taking the average of half the trace of\n",
    "$\\mathbf{G}_{avg}$ across the entire image, in accordance with the notation described in\n",
    "the subsequent figure.\n",
    "\n",
    "<figure>\n",
    "  <div style=\"display: flex; justify-content: space-between; max-width: 300px; /* Adjust this value as needed */\">\n",
    "    <div style=\"text-align: center;\">\n",
    "      <img src=\"./images/fig_7_19.png\" style=\"max-width: 100%; height: auto;\">\n",
    "      <p><strong></strong></p>\n",
    "      <p>Block diagram of the revised basic displacement estimation algorithm (DE) incorporating regularization adapted from [1].</p>\n",
    "    </div>\n",
    "  </div>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_displacement_with_regularization(A, S_T, S, delB, w, mu):\n",
    "    # Pre-calculate quantities recommended by paper\n",
    "    A_T = A.swapaxes(-1, -2)\n",
    "    ATA = S_T @ A_T @ A @ S\n",
    "    ATb = (S_T @ A_T @ delB[..., None])[..., 0]\n",
    "    # btb = delB.swapaxes(-1, -2) @ delB\n",
    "    G_avg = np.mean(ATA, axis=(0, 1))\n",
    "    h_avg = np.mean(ATb, axis=(0, 1))\n",
    "    p_avg = np.linalg.solve(G_avg, h_avg)  # fig. 7.8\n",
    "    d_avg = (S @ p_avg[..., None])[..., 0]\n",
    "\n",
    "    # Default value for mu is to set mu to 1/2 the trace of G_avg\n",
    "    if mu is None:\n",
    "        mu = 1 / 2 * np.trace(G_avg)\n",
    "\n",
    "    # Apply separable cross-correlation to calculate linear equation\n",
    "    G = correlate1d(A_T @ A, w, axis=0, mode=\"constant\", cval=0)\n",
    "    # G = correlate1d(ATA, w, axis=0, mode=\"constant\", cval=0)\n",
    "    G = correlate1d(G, w, axis=1, mode=\"constant\", cval=0)\n",
    "\n",
    "    h = correlate1d((A_T @ delB[..., None])[..., 0], w, axis=0, mode=\"constant\", cval=0)\n",
    "    # h = correlate1d(ATb, w, axis=0, mode=\"constant\", cval=0)\n",
    "    h = correlate1d(h, w, axis=1, mode=\"constant\", cval=0)\n",
    "\n",
    "    # Refine estimate of displacement field\n",
    "    # TODO: A bug when motion model is not constant\n",
    "    d = np.linalg.solve(G + mu * np.eye(2), h + mu * d_avg)\n",
    "    return d\n",
    "\n",
    "\n",
    "def estimate_displacement_without_regularization(A, S_T, S, delB, w):\n",
    "    # Pre-calculate quantities recommended by paper\n",
    "    A_T = A.swapaxes(-1, -2)\n",
    "    ATA = S_T @ A_T @ A @ S\n",
    "    ATb = (S_T @ A_T @ delB[..., None])[..., 0]\n",
    "    # btb = delB.swapaxes(-1, -2) @ delB\n",
    "\n",
    "    # If mu is 0, it means the global/average parametrized warp should not be\n",
    "    # calculated, and the parametrization should apply to the local calculations\n",
    "    # if mu == 0: # page 132\n",
    "    # Apply separable cross-correlation to calculate linear equation\n",
    "    # for each pixel: G*d = h\n",
    "    G = correlate1d(ATA, w, axis=0, mode=\"constant\", cval=0)\n",
    "    G = correlate1d(G, w, axis=1, mode=\"constant\", cval=0)\n",
    "\n",
    "    h = correlate1d(ATb, w, axis=0, mode=\"constant\", cval=0)\n",
    "    h = correlate1d(h, w, axis=1, mode=\"constant\", cval=0)\n",
    "\n",
    "    d = (S @ np.linalg.solve(G, h)[..., None])[..., 0]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Displacement Estimation\n",
    "\n",
    "Section 7.7.1 of [1] shows the simplest solution involves iterating the displacement\n",
    "estimation process three times, as illustrated in the figure below.\n",
    "\n",
    "The output displacement from one iteration serves as the a priori displacement for the\n",
    "subsequent iteration. Initially, the a priori displacement field $\\mathbf{d}_{in}$ is\n",
    "typically set to zero, unless there is actual knowledge available about the displacement\n",
    "field. The same polynomial expansion coefficients are used across all iterations and are\n",
    "required to be computed only once.\n",
    "\n",
    "The vulnerability of this method lies primarily in the first iteration. If the\n",
    "displacements (relative to the a priori displacements) are excessively large, it is\n",
    "unreasonable to anticipate improvements in the output displacements, rendering further\n",
    "iterations **ineffective**.\n",
    "\n",
    "<figure>\n",
    "  <div style=\"display: flex; justify-content: space-between; max-width: 300px; /* Adjust this value as needed */\">\n",
    "    <div style=\"text-align: center;\">\n",
    "      <img src=\"./images/fig_7_9.png\" style=\"max-width: 100%; height: auto;\">\n",
    "      <p><strong></strong></p>\n",
    "      <p>Block diagram of the iterative displacement estimation algorithm adapted from [1].</p>\n",
    "    </div>\n",
    "  </div>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_iterative(\n",
    "    f1, f2, sigma, sigma_flow, num_iter=1, d=None, model=\"constant\", mu=None\n",
    "):\n",
    "\n",
    "    # TODO: add initial warp parameters as optional input?\n",
    "\n",
    "    height, width, *_ = f1.shape\n",
    "    c1 = generate_certainty(height, width, 5)  # (height, width) float64 0.0 1.0\n",
    "    c2 = generate_certainty(height, width, 5)  # (height, width) float64 0.0 1.0\n",
    "\n",
    "    # Calculate the polynomial expansion at each point in the images\n",
    "    A1, B1, C1 = poly_exp(f1, c1, sigma)\n",
    "    A2, B2, C2 = poly_exp(f2, c2, sigma)\n",
    "\n",
    "    # Pixel coordinates of each point in the images, (height, width, 2)\n",
    "    x = np.stack(\n",
    "        np.broadcast_arrays(np.arange(f1.shape[0])[:, None], np.arange(f1.shape[1])),\n",
    "        axis=-1,\n",
    "    ).astype(int)\n",
    "\n",
    "    # Initialize displacement field\n",
    "    if d is None:\n",
    "        d = np.zeros(list(f1.shape) + [2])  # (height, width, 2)\n",
    "\n",
    "    # Set up applicability convolution window\n",
    "    n_flow = int(4 * sigma_flow + 1)\n",
    "    xw = np.arange(-n_flow, n_flow + 1)\n",
    "    w = np.exp(-(xw**2) / (2 * sigma_flow**2))\n",
    "\n",
    "    S = motion_model(x, model)\n",
    "\n",
    "    S_T = S.swapaxes(-1, -2)\n",
    "\n",
    "    # Iterate convolutions to estimate the optical flow\n",
    "    for _ in range(num_iter):\n",
    "        # Set d~ as displacement field fit to nearest pixel (and constrain to not\n",
    "        # being off image). Note we are setting certainty to 0 for points that\n",
    "        # would have been off-image had we not constrained them\n",
    "        d_ = d.astype(int)  # priori displacement\n",
    "        x_ = x + d_\n",
    "\n",
    "        # x_ = np.maximum(np.minimum(x_, np.array(f1.shape) - 1), 0)\n",
    "\n",
    "        # Constrain d~ to be on-image, and find points that would have\n",
    "        # been off-image\n",
    "        x_2 = np.maximum(np.minimum(x_, np.array(f1.shape) - 1), 0)\n",
    "        off_f = np.any(x_ != x_2, axis=-1)\n",
    "        x_ = x_2\n",
    "\n",
    "        # Set certainty to 0 for off-image points\n",
    "        c_ = c1[x_[..., 0], x_[..., 1]]\n",
    "        c_[off_f] = 0\n",
    "\n",
    "        # Calculate A and delB for each point, according to paper\n",
    "        A = (A1 + A2[x_[..., 0], x_[..., 1]]) / 2\n",
    "        A *= c_[\n",
    "            ..., None, None\n",
    "        ]  # recommendation in paper: add in certainty by applying to A and delB\n",
    "\n",
    "        delB = -1 / 2 * (B2[x_[..., 0], x_[..., 1]] - B1) + (A @ d_[..., None])[..., 0]\n",
    "        delB *= c_[\n",
    "            ..., None\n",
    "        ]  # recommendation in paper: add in certainty by applying to A and delB\n",
    "\n",
    "        if mu == 0:\n",
    "            d = estimate_displacement_without_regularization(A, S_T, S, delB, w)\n",
    "        else:\n",
    "            d = estimate_displacement_with_regularization(A, S_T, S, delB, w, mu)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-scale Displacement Estimation\n",
    "\n",
    "Section 7.7.2 in [1] introduces a multi-scale strategy for managing large displacements\n",
    "through coarse to fine scales, albeit with a trade-off in precision initially. This\n",
    "approach is illustrated in the subsequent figure.\n",
    "\n",
    "The process begins at the coarsest scale, where a preliminary, albeit rough,\n",
    "displacement estimate, $\\mathbf{d}_{\\text{in}}$, is commonly assumed to be zero in the\n",
    "absence of pre-existing displacement information. This estimate is incrementally refined\n",
    "across finer scales to achieve more accurate displacement detection. Unlike single-scale\n",
    "iterative methods, the multi-scale approach needs to recalculate polynomial expansion\n",
    "coefficients at each scale.\n",
    "\n",
    "The function `gen_gaussian_pyramids`, detailed below, create a Gaussian pyramid. Each\n",
    "image in this pyramid is represented as a floating-point number within the 0 to 1 range,\n",
    "ensuring uniformity in subsequent processing steps.\n",
    "\n",
    "<figure>\n",
    "  <div style=\"display: flex; justify-content: space-between; max-width: 300px; /* Adjust this value as needed */\">\n",
    "    <div style=\"text-align: center;\">\n",
    "      <img src=\"./images/fig_7_10.png\" style=\"max-width: 100%; height: auto;\">\n",
    "      <p><strong></strong></p>\n",
    "      <p>Block diagram of the multi-scale displacement estimation algorithm adapted from [1].</p>\n",
    "    </div>\n",
    "  </div>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_gaussian_pyramids(img_list, n_pyr):\n",
    "    \"\"\"\n",
    "    Applies Gaussian pyramid transformations to a list of images, zips the transformed images together,\n",
    "    and then reverses the order of the resulting list.\n",
    "\n",
    "    Parameters:\n",
    "    - img_list: List of images to transform. Each image should be compatible with skimage.transform.pyramid_gaussian.\n",
    "    - n_pyr: The number of pyramid layers to use in the transformation.\n",
    "\n",
    "    Returns:\n",
    "    - A reversed list of the zipped, pyramid-transformed images.\n",
    "    \"\"\"\n",
    "    # Apply the Gaussian pyramid transformation to each image in the list with the specified number of layers\n",
    "    transformed_images = list(\n",
    "        map(partial(skimage.transform.pyramid_gaussian, max_layer=n_pyr), img_list)\n",
    "    )\n",
    "\n",
    "    # Zip the transformed images together and reverse the order\n",
    "    zipped_and_reversed = reversed(list(zip(*transformed_images)))\n",
    "\n",
    "    return zipped_and_reversed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "After defining all the components of Farnebäck's algorithm, we can encapsulate the\n",
    "entire process into a single function. The `calc_optical_flow_farneback` function takes\n",
    "two images as inputs and computes the optical flow between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_optical_flow_farneback(img1, img2):\n",
    "    assert img1.shape == img2.shape\n",
    "\n",
    "    n_pyr = 3\n",
    "    opts = dict(\n",
    "        sigma=4.0,\n",
    "        sigma_flow=4.0,\n",
    "        num_iter=3,\n",
    "        model=\"constant\",  # TODO: Only constant works.\n",
    "        mu=None,  # if mu is zero, there will be singularity error in linalg error. Use None and let it be computed automatically\n",
    "    )\n",
    "\n",
    "    d = None  # optical flow field\n",
    "\n",
    "    gaussion_pyramids = gen_gaussian_pyramids([img1, img2], n_pyr=n_pyr)\n",
    "    for pyr1, pyr2 in gaussion_pyramids:\n",
    "        if d is not None:\n",
    "            d = skimage.transform.pyramid_expand(d, channel_axis=-1)\n",
    "            d = d[: pyr1.shape[0], : pyr2.shape[1]] * 2\n",
    "            # TODO: d and pyr1 shape may not match due to downsampling/upsampling.\n",
    "            assert d.shape[0] == pyr1.shape[0] and d.shape[1] == pyr1.shape[1]\n",
    "\n",
    "        d = flow_iterative(pyr1, pyr2, d=d, **opts)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "cap = cv2.VideoCapture(cv2.samples.findFile(\"input/snatch.mp4\"))  # 25 fps\n",
    "ret, frame1 = cap.read()\n",
    "prev = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "# prev: (1080, 1920) uint8 0 255\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[..., 1] = 255\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec\n",
    "print(\"cap.get(cv2.CAP_PROP_FPS)\", cap.get(cv2.CAP_PROP_FPS))\n",
    "out = cv2.VideoWriter(\n",
    "    \"./output/snatch_optical_flow.mp4\",\n",
    "    fourcc,\n",
    "    fps=cap.get(cv2.CAP_PROP_FPS),\n",
    "    frameSize=(frame1.shape[1], frame1.shape[0]),\n",
    ")\n",
    "\n",
    "stop = 1\n",
    "while 1:\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret or stop == 0:\n",
    "        print(\"No frames grabbed!\")\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # next: (1080, 1920) uint8 0 255\n",
    "\n",
    "    flow = calc_optical_flow_farneback(prev, next)\n",
    "    # flow: (1080, 1920, 2) float64\n",
    "\n",
    "    bgr = my_utils.flow_to_color(flow, hsv)\n",
    "\n",
    "    if DEBUG:\n",
    "        cv2.imwrite(\"./output_farneback.png\", bgr)\n",
    "        stop -= 1\n",
    "        print(\"stop: \", stop)\n",
    "    else:\n",
    "        out.write(bgr)\n",
    "\n",
    "        # Show the result on the fly\n",
    "        # cv2.imshow(\"frame2\", bgr)\n",
    "        # k = cv2.waitKey(30) & 0xFF\n",
    "        # if k == 27:\n",
    "        #     break\n",
    "        # elif k == ord(\"s\"):\n",
    "        #     cv2.imwrite(\"opticalfb.png\", frame2)\n",
    "        #     cv2.imwrite(\"opticalhsv.png\", bgr)\n",
    "\n",
    "    prev = next\n",
    "\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_computer_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
