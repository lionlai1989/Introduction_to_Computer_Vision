{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction of 3D Models from Object Silhouettes\n",
    "\n",
    "The process of reconstructing 3D models can be broadly categorized into two classes of\n",
    "algorithms. The first class involves the computation of disparity and depth maps from\n",
    "multiple views of an object, which are then registered to create a single 3D model. In\n",
    "contrast, the subject of this tutorial revolves around the second class of\n",
    "reconstruction algorithms, which embrace a volumetric-based methodology to depict the\n",
    "scene. This approach is commonly referred to as **'Shape-from-Silhouette'**.\n",
    "\n",
    "To illustrate, let's consider the video below, where an object is captured from 18\n",
    "different angles, providing the essential data for constructing our 3D models. In its\n",
    "simplest form, volumetric-based 3D reconstruction can be described as follows:\n",
    "Initially, the space encompassing the object to be reconstructed is divided into small\n",
    "cubic volumes called voxels. Each voxel is projected onto image coordinates. If a\n",
    "projected voxel falls within the contour, it is considered a part of the object;\n",
    "conversely, a voxel outside the contour is not regarded as belonging to the object.\n",
    "\n",
    "Now, let's dive into the detail of shape-from-silhouette.\n",
    "\n",
    "**RULES:** As usual, **`OpenCV`** is banned in this repository.\n",
    "\n",
    "**Acknowledgment:** Please note that this tutorial will follow the methodology used in\n",
    "\"Reconstruction of Volumetric 3D Models\" by Peter Eisert, allowing readers to easily\n",
    "reference the source for additional information when needed. Additionally, the majority\n",
    "of the figures have been adapted from the same paper.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"./images/david_silhouette.gif\">\n",
    "  <p style=\"font-size: 14px; color: #777;\"></p>\n",
    "  <p>A statue is captured from 18 different angles.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdcution\n",
    "\n",
    "The fundamental concept of shape-from-silhouette revolves around the idea that any\n",
    "object must entirely fit within its contour. When an object is observed from a known\n",
    "perspective under a perspective projection, the rays originating from the focal point\n",
    "and passing through the silhouette contour create what we can call the hull of a viewing\n",
    "cone. This viewing cone, as depicted on the left-hand side of the figure below, sets an\n",
    "upper boundary for the object's shape. The actual volume of the object is guaranteed to\n",
    "be less than or equal to this rough approximation. Importantly, no specific assumptions\n",
    "are made regarding the viewing position, except for the camera calibration data.\n",
    "\n",
    "For any given viewing position, the silhouette defines a viewing cone that fully\n",
    "encompasses the object. As the object's volume is constrained by all the individual\n",
    "cones, it is constrained to reside within the intersection of these viewing cones. The\n",
    "right-hand side of the figure below illustrates this concept, where only points in the\n",
    "3D space that lie inside all viewing cones are considered part of the object to be\n",
    "reconstructed.\n",
    "\n",
    "However, it's important to note that shape-from-silhouette methods have limitations.\n",
    "They cannot reconstruct all possible shapes. For instance, consider the concavity shown\n",
    "on the right-hand side of the figure below â€“ this feature is never visible in the\n",
    "silhouette and, consequently, cannot be recovered. Rather than obtaining the true shape,\n",
    "the method can only provide an estimation of the\n",
    "[visual hull](https://en.wikipedia.org/wiki/Visual_hull).\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"./images/single_viewing_cone.png\" height=\"256\">\n",
    "  <img src=\"./images/multiple_viewing_cones.png\" height=\"256\">\n",
    "  <p style=\"font-size: 14px; color: #777;\"></p>\n",
    "  <p>Left: Viewing cone through the image silhouette containing the object. Right: Intersection of multiple viewing cones.</p>\n",
    "</div>\n",
    "\n",
    "Thus, the entire procedure of shape-from-silhouette can be summarized in the following\n",
    "steps:\n",
    "\n",
    "-   Calibration of the cameras to determine their position, orientation, and intrinsic\n",
    "    camera parameters.\n",
    "-   Segmentation of the object from the background in the captured images to derive the\n",
    "    object's contour.\n",
    "-   Estimation of the bounding volume in 3D world coordinates.\n",
    "-   Projecting voxels into image coordinates and accumulating valid voxels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Calibration and Camera Projection Matrix\n",
    "\n",
    "The camera projection matrix is derived from the process of camera calibration, which\n",
    "entails extracting pairs of points in 3D world coordinates and their corresponding 2D\n",
    "image coordinates. Fortunately, this information has been provided in this example,\n",
    "allowing us to simply read the camera projection matrix as shown below. `images`\n",
    "contains 18 grayscale images, and `camera_matrices` holds the associated camera\n",
    "projection matrices for these 18 images.\n",
    "\n",
    "Note: If you want to know more about camera calibration, please visit this\n",
    "[tutorial](https://htmlpreview.github.io/?https://github.com/lionlai1989/Introduction_to_Computer_Vision/blob/master/Stereo_Geometry/Stereo_Geometry.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"./input/\")\n",
    "images = []\n",
    "camera_matrices = []\n",
    "\n",
    "for img_path, mat_path in zip(\n",
    "    natsorted([f for f in data_folder.glob(\"*.jpg\")]),\n",
    "    natsorted([f for f in data_folder.glob(\"*.pa\")]),\n",
    "):\n",
    "    assert img_path.stem == mat_path.stem\n",
    "    img = io.imread(img_path, as_gray=True)\n",
    "    img *= 255.0\n",
    "    img[img >= 255] = 255\n",
    "    img = img.astype(np.uint8)\n",
    "    assert img.ndim == 2\n",
    "    images.append(img)\n",
    "    camera_matrices.append(np.genfromtxt(mat_path))\n",
    "\n",
    "assert images[0].shape == (480, 640) and camera_matrices[0].shape == (3, 4)\n",
    "num_views = len(images)\n",
    "print(\n",
    "    f\"We first convert images from RGB to grayscale.\\n\"\n",
    "    + f\"Each image's size is {images[0].shape} and its range is from {np.min(img)} to {np.max(img)}.\\n\"\n",
    "    + f\"Finally, there is an associated {camera_matrices[0].shape} projection matrix for each image.\\n\"\n",
    "    + f\"There are {num_views} views.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation of the Object from the Background\n",
    "\n",
    "Now, our objective is to generate silhouettes for the statue in these 18 images,\n",
    "effectively segmenting the statue from the background. The initial step involves\n",
    "creating binary silhouette images from the original images. Once again, we are fortunate\n",
    "in this example. With the white statue against a black background, we can apply a simple\n",
    "threshold `threshold` to create a binary mask, denoted as `mask`. In this mask, statue\n",
    "pixels are represented as `1`, while non-statue pixels are denoted as `0`. Consequently,\n",
    "the `silhouettes` variable is a list containing 18 binary masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 101\n",
    "\n",
    "num_col, num_row = 6, 3\n",
    "fig, ax = plt.subplots(nrows=num_row, ncols=num_col, figsize=(33, 13))\n",
    "silhouettes = []\n",
    "for row in range(num_row):\n",
    "    for col in range(num_col):\n",
    "        mask = images[row * num_col + col] >= threshold\n",
    "        silhouettes.append(mask)\n",
    "        ax[row, col].imshow(mask, cmap=\"gray\")\n",
    "        ax[row, col].set_axis_off()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of the bounding volume in 3D world coordinates\n",
    "\n",
    "In the following steps, our objective is to estimate the bounding volume in 3D world\n",
    "coordinates, which signifies the space we intend to project into 2D image coordinates.\n",
    "This process involves working with three distinct sets of coordinates:\n",
    "\n",
    "-   **2D image coordinates**: These are the coordinates in the image itself.\n",
    "-   **3D world coordinates**: Representing the 3D space in the real world.\n",
    "-   **3D volume coordinates**: Referring to coordinates within the volumetric space.\n",
    "\n",
    "Our primary objective is to project the position of a voxel from 3D volume coordinates\n",
    "to 3D world coordinates. This transformation allows us to locate the voxel in the\n",
    "real-world 3D space. Subsequently, we project this 3D world coordinate into 2D image\n",
    "coordinates, as illustrated in the image below.\n",
    "\n",
    "The projection matrix from 3D world coordinates to 2D image coordinates is already\n",
    "provided as part of the assignment. Our current task is to determine how to construct\n",
    "the projection matrix for transforming 3D volume coordinates into 3D world coordinates.\n",
    "This transformation is accomplished through the `volume_to_world` function. It's worth\n",
    "noting that the y-axis undergoes a flip during this process.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"./images/project_volume.png\" height=\"512\">\n",
    "  <p style=\"font-size: 14px; color: #777;\"></p>\n",
    "  <p>Representation of a scene by a 3D array of volume elements - voxels.</p>\n",
    "</div>\n",
    "\n",
    "After conducting experiments, I've determined that the 3D volume coordinates can be\n",
    "defined as follows in Python:\n",
    "\n",
    "```python\n",
    "volume_x = int(64)\n",
    "volume_y = int(64)\n",
    "volume_z = int(256)\n",
    "```\n",
    "\n",
    "These values set the dimensions for the 3D volume, with `volume_x`, `volume_y`, and\n",
    "`volume_z` representing the size in the x, y, and z dimensions, respectively.\n",
    "\n",
    "Additionally, the 3D world coordinates can be represented using a 2x3 NumPy array,\n",
    "`bbox_3d`, as follows:\n",
    "\n",
    "```python\n",
    "bbox_3d = np.array([[0.3, -0.1, -1.9], [2.1, 1.3, 2.5]])\n",
    "```\n",
    "\n",
    "In this 2x3 array, each column specifies the starting and ending points for each axis.\n",
    "For example, the x-axis spans from 0.3 to 2.1, the y-axis from -0.1 to 1.3, and the\n",
    "z-axis from -1.9 to 2.5.\"\n",
    "\n",
    "We proceed by creating `volume_3d_corners`, which represents the eight corners of the 3D\n",
    "volume in volume coordinates. Our goal is to project each of these corners into image\n",
    "coordinates. However, it's important to note that this projection results in the loss of\n",
    "depth information in the image coordinates. As a result, we obtain eight corresponding\n",
    "points in 2D image coordinates. To determine the smallest bounding box that encompasses\n",
    "these eight points, we utilize the `get_2D_bbox` function.\n",
    "\n",
    "As illustrated in the graph below, a 2D bounding box is depicted on each 2D image, and\n",
    "each of these bounding boxes effectively encapsulates the entire statue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volume_to_world(bbox, x_length, y_length, z_length):\n",
    "    res_x = (bbox[1, 0] - bbox[0, 0]) / x_length\n",
    "    res_y = (bbox[1, 1] - bbox[0, 1]) / y_length\n",
    "    res_z = (bbox[1, 2] - bbox[0, 2]) / z_length\n",
    "    eye = np.array(\n",
    "        [\n",
    "            [1, 0, 0, bbox[0, 0]],\n",
    "            [0, 1, 0, bbox[0, 1]],\n",
    "            [0, 0, 1, bbox[0, 2]],\n",
    "            [0, 0, 0, 1],\n",
    "        ]\n",
    "    )\n",
    "    v_to_w = np.dot(eye, np.diag([res_x, res_y, res_z, 1]))\n",
    "    flip_y = np.array([[1, 0, 0, 0], [0, 0, 1, 0], [0, -1, 0, 0], [0, 0, 0, 1]])\n",
    "    return np.dot(flip_y, v_to_w)\n",
    "\n",
    "\n",
    "def get_2D_bbox(normalized_2d_pts):\n",
    "    _, num_pts = normalized_2d_pts.shape\n",
    "    min_x, min_y, max_x, max_y = np.inf, np.inf, -np.inf, -np.inf\n",
    "    for i in range(num_pts):\n",
    "        min_x = min(min_x, normalized_2d_pts[0, i])\n",
    "        min_y = min(min_y, normalized_2d_pts[1, i])\n",
    "        max_x = max(max_x, normalized_2d_pts[0, i])\n",
    "        max_y = max(max_y, normalized_2d_pts[1, i])\n",
    "    return min_x, min_y, max_x, max_y\n",
    "\n",
    "\n",
    "bbox_3d = np.array([[0.3, -0.1, -1.9], [2.1, 1.3, 2.5]])\n",
    "volume_x = int(64)\n",
    "volume_y = int(64)\n",
    "volume_z = int(256)\n",
    "proj_volume2world = volume_to_world(bbox_3d, volume_x, volume_y, volume_z)\n",
    "volume_3d_corners = np.array(\n",
    "    [\n",
    "        [0, 0, 0, 1],\n",
    "        [0, 0, volume_z, 1],\n",
    "        [0, volume_y, 0, 1],\n",
    "        [0, volume_y, volume_z, 1],\n",
    "        [volume_x, 0, 0, 1],\n",
    "        [volume_x, 0, volume_z, 1],\n",
    "        [volume_x, volume_y, 0, 1],\n",
    "        [volume_x, volume_y, volume_z, 1],\n",
    "    ]\n",
    ").T  # (4, 8)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=num_row, ncols=num_col, figsize=(33, 13))\n",
    "for row in range(num_row):\n",
    "    for col in range(num_col):\n",
    "        cam = camera_matrices[row * num_col + col]\n",
    "        ax[row, col].imshow(images[row * num_col + col], cmap=\"gray\")\n",
    "        bbox_2d = np.dot(cam, np.dot(proj_volume2world, volume_3d_corners))\n",
    "        normalized_pts = np.divide(bbox_2d, bbox_2d[2, :])\n",
    "        min_x, min_y, max_x, max_y = get_2D_bbox(normalized_pts)\n",
    "        x, y, w, h = min_x, min_y, max_x - min_x, max_y - min_y\n",
    "        bounding_box = patches.Rectangle(\n",
    "            (x, y), w, h, linewidth=1, edgecolor=\"y\", facecolor=\"none\"\n",
    "        )\n",
    "        ax[row, col].add_patch(bounding_box)  # Add the bounding box to the plot\n",
    "        ax[row, col].set_axis_off()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voxel Projection and Accumulation\n",
    "\n",
    "After establishing the 3D volume in world coordinates, the subsequent critical step\n",
    "involves projecting the voxels into 2D image coordinates. For each voxel with a known 3D\n",
    "position, we calculate its projection onto 2D image coordinates. Voxels that fall\n",
    "outside the silhouette in at least one view are removed from the volume. In contrast,\n",
    "voxels that fall within the silhouette for all views are awarded one vote each. After\n",
    "processing all the voxels, we derive a 3D voxel array. Voxels with precisely 18 votes\n",
    "are recognized as part of the statue. The reason for this specific vote count is that we\n",
    "have 18 different views of the statue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_votes = np.zeros((volume_x, volume_y, volume_z))\n",
    "\n",
    "volume_coordinates = []\n",
    "for i in range(volume_x):\n",
    "    for j in range(volume_y):\n",
    "        for k in range(volume_z):\n",
    "            volume_coordinates.append((i, j, k, 1))\n",
    "volume_coordinates = np.stack(volume_coordinates, axis=0).T\n",
    "assert volume_votes.shape == (volume_x, volume_y, volume_z)\n",
    "assert volume_coordinates.shape == (4, volume_x * volume_y * volume_z)\n",
    "\n",
    "for cam, mask in zip(camera_matrices, silhouettes):\n",
    "    _, num_points = volume_coordinates.shape\n",
    "    bbox_2d = np.dot(cam, np.dot(proj_volume2world, volume_coordinates))\n",
    "    normalized = np.divide(bbox_2d, bbox_2d[2, :])\n",
    "    for i in range(num_points):\n",
    "        u, v = int(normalized[0, i]), int(normalized[1, i])\n",
    "        if mask[v, u] == 1:\n",
    "            volume_votes[\n",
    "                volume_coordinates[0, i],\n",
    "                volume_coordinates[1, i],\n",
    "                volume_coordinates[2, i],\n",
    "            ] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the Accumulated Voxels Array\n",
    "\n",
    "The last thing we need to do is to visualize the accumulated voxels array in a rotating\n",
    "3D view. To achieve this, we create a view for each rotation angle and save it as\n",
    "individual frames. While creating a 3D animation is an option, for this purpose, we'll\n",
    "focus on generating static views for each rotation angle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_array = volume_votes == num_views\n",
    "assert voxel_array.shape == (volume_x, volume_y, volume_z)\n",
    "\n",
    "rotation_speed_deg_per_sec = 20  # Define the rotation parameters\n",
    "\n",
    "# Create images for each frame\n",
    "for frame in range(num_views):\n",
    "    # Create `fig` here. Calling `ax` multiple times exploits the memory.\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.set_axis_off()\n",
    "    ax.set_box_aspect((volume_x, volume_y, volume_z))\n",
    "    ax.set_facecolor(\"silver\")  # Set the background color\n",
    "\n",
    "    angle_deg = frame * rotation_speed_deg_per_sec\n",
    "    filename = f\"./output/david_3d_{frame}.png\"\n",
    "    print(filename)\n",
    "    ax.voxels(voxel_array, facecolors=\"white\", edgecolor=\"k\", linewidth=0.1)\n",
    "    ax.view_init(\n",
    "        elev=20.0, azim=int((270 + angle_deg) % 360)\n",
    "    )  # Sync the angle with the input \"david_**.jpg\"\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(filename, format=\"png\", bbox_inches=\"tight\", pad_inches=0, dpi=200)\n",
    "\n",
    "    # Close the figure\n",
    "    plt.close(fig)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:**\n",
    "\n",
    "-   \"Reconstruction of Volumetric 3D Models\" by Peter Eisert.\n",
    "-   \"Multi-hypothesis, volumetric reconstruction of 3-D objects from multiple calibrated\n",
    "    camera views\" by Peter Eisert, Eckehard Steinbach, and Bernd Girod.\n",
    "-   \"Automatic 3D Model Construction for Turn-Table Sequences\" by Andrew W. Fitzgibbon,\n",
    "    Geoff Cross, and Andrew Zisserman.\n",
    "-   [Animate a rotating 3D graph in matplotlib](https://stackoverflow.com/questions/18344934/animate-a-rotating-3d-graph-in-matplotlib)\n",
    "-   [how to set \"camera position\" for 3d plots using python/matplotlib?](https://stackoverflow.com/questions/12904912/how-to-set-camera-position-for-3d-plots-using-python-matplotlib)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_computer_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
