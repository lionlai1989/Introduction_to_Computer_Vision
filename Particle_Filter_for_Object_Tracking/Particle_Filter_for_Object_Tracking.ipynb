{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Filter for Object Tracking\n",
    "\n",
    "The particle filter is a general algorithm capable of addressing various problem types,\n",
    "with a particular strength in solving estimation problems. It excels in estimating the\n",
    "states of systems with _multimodal_ states, a task that conventional Kalman filters,\n",
    "including the _classic Kalman filter_, _extended Kalman filter_, and _unscented Kalman\n",
    "filter_, struggle to handle effectively. In this tutorial, I will explore the\n",
    "application of the particle filter for object tracking through two illustrative\n",
    "examples. The first example can be addressed using algorithms from the Kalman filter\n",
    "family, whereas the second problem demands the unique capabilities of the particle\n",
    "filter.\n",
    "\n",
    "**RULES:** As usual, **`OpenCV`** is banned in this repository.\n",
    "\n",
    "Please note that this tutorial will adhere to the notation used in the\n",
    "[Particle Filter's Wikipedia page](https://en.wikipedia.org/wiki/Particle_filter),\n",
    "ensuring that readers can easily refer to the source for additional information when\n",
    "needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import cv2\n",
    "import time\n",
    "import imageio\n",
    "import numpy as np\n",
    "from skimage import color\n",
    "import matplotlib.pyplot as plt\n",
    "from particle_filter_utils import *\n",
    "from functools import partial\n",
    "from skimage import transform\n",
    "\n",
    "video_reader = imageio.get_reader(\"./input/pres_debate.avi\")\n",
    "frames = [np.array(frame) for frame in video_reader]\n",
    "video_reader.close()\n",
    "president_video = np.array(frames)\n",
    "assert president_video.ndim == 4  # n x H x W x 3\n",
    "\n",
    "video_reader = imageio.get_reader(\"./input/pedestrians.avi\")\n",
    "frames = [np.array(frame) for frame in video_reader]\n",
    "video_reader.close()\n",
    "blonde_video = np.array(frames)\n",
    "assert blonde_video.ndim == 4  # n x H x W x 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial focuses on the application of a particle filter for tracking objects over\n",
    "time. It's essential to clarify that we are specifically addressing a tracking problem,\n",
    "not a detection problem. In other words, our objective is not to detect an object but to\n",
    "track it once it's already known to us. To illustrate this, consider the scenario where\n",
    "we have initial information about a car's position, and our goal is to autonomously\n",
    "monitor and update the car's position continuously throughout a given time period.\n",
    "\n",
    "When tracking an object, the primary objective is to deduce a sequence of world states,\n",
    "denoted as $x_k$, from a noisy sequence of measurements or observations, denoted as\n",
    "$y_k$. Particle filters share similarities with Kalman filters, as they consist of two\n",
    "essential components: the dynamic or temporal model, denoted as $g(\\cdot)$, and the\n",
    "measurement model, denoted as $h(\\cdot)$.\n",
    "\n",
    "-   The dynamic or temporal model, $g(\\cdot)$, characterizes the relationship between\n",
    "    successive states. Typically, particle filters make use of the Markov assumption,\n",
    "    which posits that each state depends solely on its predecessor, represented as\n",
    "    $P(x_k | x_{k-1})$.\n",
    "\n",
    "-   The measurement model, $h(\\cdot)$, describes the connection between the measurement\n",
    "    $y_k$ and the state $x_k$ at time $k$. We consider this model as generative, and it\n",
    "    helps us model the likelihood, $P(y_k | x_k)$.\n",
    "\n",
    "By leveraging this statistical dependency, we can infer the state, $x_k$, even when the\n",
    "associated observation, $y_k$, provides partial or no informative content.\n",
    "\n",
    "In the context of inference, the primary challenge is to calculate the marginal\n",
    "posterior distribution:\n",
    "\n",
    "$$\n",
    "P(x_k|y_{0\\dots k}) = \\frac{P(y_k|x_k)P(x_k|y_{0\\dots {k-1}})}{\\int P(y_k|x_k)\n",
    "P(x_k|y_{0 \\dots {k-1}}) dx}\n",
    "$$\n",
    "\n",
    "To evaluate $P(x_k | y_{0\\dots k})$, we need to determine $P(x_k | y_{0\\dots {k-1}})$,\n",
    "which signifies our prior knowledge about the state $x_k$ before incorporating the\n",
    "associated measurement $y_k$. This can be computed as follows:\n",
    "\n",
    "$$\n",
    "P(x_k|y_{0 \\dots k-1}) = \\int P(x_k|x_{k-1}) P(x_{k-1}|y_{0 \\dots {k-1}}) dx_{k-1}\n",
    "$$\n",
    "\n",
    "One of the simplest particle filter methods is the _conditional density propagation_ or\n",
    "_condensation_ algorithm. In this algorithm, the probability distribution\n",
    "$P(x_k|y_{0\\dots k-1})$ is represented by a weighted sum of particles. The following\n",
    "intuitive image provides an overview of how the condensation algorithm works:\n",
    "\n",
    "a) The posterior at the previous step is represented as a set of weighted particles.\n",
    "\n",
    "b) The particles are resampled according to their weights to produce a new set of\n",
    "unweighted particles.\n",
    "\n",
    "c) These particles are passed through the nonlinear temporal function.\n",
    "\n",
    "d) Noise is added according to the temporal model.\n",
    "\n",
    "e) The particles are passed through the measurement model and compared to the\n",
    "measurement density.\n",
    "\n",
    "f) The particles are re-weighted according to their compatibility with the measurements,\n",
    "and the process can begin again.\n",
    "\n",
    "<figure>\n",
    "  <div style=\"display: flex; justify-content: space-between;\">\n",
    "    <div style=\"text-align: center;\">\n",
    "      <img src=\"./images/Particle.svg\">\n",
    "      <p><strong>The condensation algorithm </strong></p>\n",
    "      <p>The image and the accompanying description provided above have been adapted from the book \"Computer Vision: Models, Learning, and Inference\" authored by Simon J. D. Prince.</p>\n",
    "    </div>\n",
    "  </div>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navie Particle Filter\n",
    "\n",
    "After providing an overview of our objective, let's delve into the specific details of\n",
    "our task. To begin, it's crucial to establish the definition of our \"model\" or\n",
    "\"template\" within this context. The \"model\" or \"template\" represents the object that we\n",
    "aim to track. This entity could manifest as a patch in an image, a contour, or any other\n",
    "descriptive representation of the object under consideration.\n",
    "\n",
    "For the first task, we need to track a patch taken from the first frame of the video as\n",
    "shown below, which is Mitt Romney's face. Our goal is to track this face throughout the\n",
    "time. Thus we can define a `Template` which holds the information of the template as\n",
    "follow:\n",
    "\n",
    "```python\n",
    "class Template:\n",
    "    def __init__(self, img, x, y, w, h) -> None:\n",
    "        self.x = int(x)\n",
    "        self.y = int(y)\n",
    "        self.w = int(w)\n",
    "        self.h = int(h)\n",
    "        self.model = img[self.y : self.y + self.h, self.x : self.x + self.w, ...]\n",
    "```\n",
    "\n",
    "Once we've established our template, it naturally leads to the design of the system's\n",
    "state, which comprises the following four components: `(x, y, w, h)`. These components\n",
    "represent the x-axis coordinate `x`, the y-axis coordinate `y`, the width of the window\n",
    "`w`, and the height of the window `h`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_frame = president_video[0, ...]  # NOTE: uint8. 0-255.\n",
    "print(\"first_frame: \", first_frame.shape)\n",
    "x, y, w, h = 320, 175, 103, 129\n",
    "template = Template(first_frame, x, y, w, h)\n",
    "print(\n",
    "    \"template.model:\",\n",
    "    \"shape:\", template.model.shape,\n",
    "    \"type:\", template.model.dtype,\n",
    "    \"min:\", np.min(template.model),\n",
    "    \"max:\", np.max(template.model),\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 2))  # Adjust the figure size as needed\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "ax.imshow(template.model)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Model\n",
    "\n",
    "Now that we have defined our system's state, we can proceed to define our dynamic\n",
    "system. In this straightforward particle filter, we opt for the simplest form of motion,\n",
    "known as Brownian motion:\n",
    "\n",
    "$$\n",
    "x_{k} = x_{t-1} + W_{k-1}\n",
    "$$\n",
    "\n",
    "The process noise represented by $W_{k-1}$ introduces variability in our dynamic model.\n",
    "It's important to note that the range and magnitude of each dimension in the state and\n",
    "the associated process noise can differ. For instance, when considering `x` and `y`,\n",
    "these dimensions should remain within the boundaries of the image's dimensions.\n",
    "Consequently, the standard deviation of their process noise should typically fall within\n",
    "the range of 10 to 50. On the other hand, `w` and `h` should fall within an anticipated\n",
    "range, typically around 10 to 25 pixels added or subtracted from the original patch\n",
    "size. This choice is made based on practical considerations, as we wouldn't expect an\n",
    "object, such as Mitt Romney's face, to occupy the entire image. Hence, the standard\n",
    "deviation of the process noise of `w` and `h` should be only few pixels.\n",
    "\n",
    "The choice of using Brownian motion as the dynamic model is grounded in the absence of\n",
    "external control inputs to govern the object being tracked. In such scenarios, we can\n",
    "assume that the object's movement is inherently random. The implementation is\n",
    "straightforward, as outlined below. Essentially, during the prediction step, we\n",
    "introduce Gaussian noise to the state vector. You might wonder about the magnitude of\n",
    "noise added to the state. The magnitude varies depending on the specific state\n",
    "dimension. For instance, if we have prior knowledge that the object in the image\n",
    "typically moves only a few pixels in each time step, an appropriate value might range\n",
    "from a sub ten to tens of pixels. It's important to note that in our task, we must\n",
    "constrain the boundaries of the state, as it wouldn't be logical for image coordinates,\n",
    "for example, to be negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicModel:\n",
    "    def __init__(self, std: List) -> None:\n",
    "        self.std = std\n",
    "        self.num_states = len(std)\n",
    "\n",
    "    def predict(self, particles: np.ndarray, state_boundry: List[Tuple]):\n",
    "        for i in range(self.num_states):\n",
    "            # Brownian motion.\n",
    "            particles[:, i] += np.random.normal(\n",
    "                loc=0, scale=self.std[i], size=particles[:, i].shape\n",
    "            )  # Modify particles in-place\n",
    "\n",
    "            lower_bound, upper_bound = state_boundry[i]\n",
    "\n",
    "            # Constrain state within boundry.\n",
    "            np.clip(particles[:, i], lower_bound, upper_bound, out=particles[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement Model\n",
    "\n",
    "Now, let's delve into the components of the measurement model. We'll discuss each aspect\n",
    "one by one:\n",
    "\n",
    "#### Measurement function:\n",
    "\n",
    "-   The measurement function is defined as:\n",
    "\n",
    "    $$\n",
    "    \\text{weights} \\propto p(y_k|x_k) \\propto \\text{exp}({\\frac{-\\text{distance}}{2\\sigma}})\n",
    "    $$\n",
    "\n",
    "    In this context, the term \"distance\" always remains positive, representing the\n",
    "    dissimilarity between a window and the template. If $\\text{distance} = 0$, it\n",
    "    indicates that the window and the template are identical. As the value of\n",
    "    $\\text{distance}$ increases, it signifies a greater dissimilarity between the window\n",
    "    and the template. The expression $e^{-x}$ always yields values between 0 and 1,\n",
    "    which proves to be quite useful. The parameter $\\sigma$ represents the standard\n",
    "    deviation associated with our observation data. A larger $\\sigma$ implies greater\n",
    "    uncertainty in our observation data, while a smaller $\\sigma$ implies a higher\n",
    "    degree of trust in the observation data.\n",
    "\n",
    "-   Universal Sigma: The question arises as to whether a universal standard deviation\n",
    "    can be used for different types of distance functions. It is arguable that a\n",
    "    universal standard deviation can be applied if the distance is properly normalized.\n",
    "\n",
    "-   Distance Functions: I have explored three distance functions:\n",
    "    [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error),\n",
    "    [Chi-Squared](https://stats.stackexchange.com/questions/184101/comparing-two-histograms-using-chi-square-distance),\n",
    "    and [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity).\n",
    "\n",
    "    In this context, it's important to note that Mean Squared Error and Chi-Squared\n",
    "    effectively measure dissimilarity between two images, while Cosine Similarity\n",
    "    quantifies similarity. To adapt similarity to dissimilarity, I've used the\n",
    "    transformation $2 - (n + 1)$. However, it's worth mentioning that only Mean Squared\n",
    "    Error and Chi-Squared are effective in this context, as they align with the\n",
    "    objective of measuring dissimilarity between the template and the observed window.\n",
    "    Cosine Similarity, on the other hand, is not effective in this task. Additionally, I\n",
    "    initially mentioned the idea of normalizing the distance, but upon experimentation,\n",
    "    I found that applying normalization to Chi-Squared did not yield valid results.\n",
    "\n",
    "-   In the particle filter, it's important to obtain windows corresponding to each\n",
    "    particle. Each particle is characterized by four states: x-coordinate, y-coordinate,\n",
    "    width, and height. These four parameters, (x, y, w, h), are used to crop a window\n",
    "    from the given frame. There are several considerations in this process: First,\n",
    "    although we've constrained the states' minimum and maximum values during the\n",
    "    resampling stage, it's still not guaranteed that (x, y, w, h) will always define a\n",
    "    valid window. To ensure that (x, y, w, h) represents a valid window, we would need\n",
    "    to constrain the relationships between x and w, as well as y and h. However, it's\n",
    "    not always necessary to impose these constraints, as we can identify invalid windows\n",
    "    in later steps and reset the weight of a particle to 0 if necessary. Second, the\n",
    "    cropped window obtained using (x, y, w, h) might have a different size than the\n",
    "    template. To match the template's size, we need to resize the cropped window\n",
    "    accordingly. Finally, it's essential to normalize the weights assigned to particles\n",
    "    so that they sum up to 1. This normalization ensures that the particle weights\n",
    "    represent a valid probability distribution.\n",
    "\n",
    "-   Updating the template is a crucial aspect of object tracking, especially when the\n",
    "    target's appearance can change over time. The template can be updated using the\n",
    "    following equation:\n",
    "\n",
    "    $$\n",
    "    \\text{template} = \\alpha * \\text{state} + (1 - \\alpha) * \\text{template}\n",
    "    $$\n",
    "\n",
    "    This update process involves blending the latest state with the historical template.\n",
    "    However, it's important to exercise caution when performing these updates. While\n",
    "    it's often beneficial to update the template with the latest state to adapt to\n",
    "    changes, there are situations, such as occlusion, where all states should be\n",
    "    considered invalid. In these cases, our current approach could lead to the\n",
    "    generation of a relatively better state, which may not be desirable. Hence, when\n",
    "    updating the template, careful consideration is required to ensure that the update\n",
    "    process accurately reflects the evolving appearance of the target while accounting\n",
    "    for potential inaccuracies and occlusion scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeasurementModel:\n",
    "    def __init__(self, std, template, distance_func, alpha=0.0) -> None:\n",
    "        self.std = std\n",
    "        self.template = template\n",
    "        self.distance_func = distance_func\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def measure(self, particles, frame):\n",
    "        num_particles = particles.shape[0]\n",
    "\n",
    "        # Get windows corresponding to each particle. Fixiate the size of template and\n",
    "        # resize the window of each particle to match template's.\n",
    "        template_width, template_height = self.template.w, self.template.h\n",
    "        artificial_measurements = []\n",
    "        for i in range(num_particles):\n",
    "            x = particles[i, 0]  # float\n",
    "            y = particles[i, 1]  # float\n",
    "            w = particles[i, 2]  # float\n",
    "            h = particles[i, 3]  # float\n",
    "            start_y = int(y - h / 2)\n",
    "            start_x = int(x - w / 2)\n",
    "            end_y = int(start_y + h)\n",
    "            end_x = int(start_x + w)\n",
    "            temp = frame[start_y:end_y, start_x:end_x, :]\n",
    "            if temp.size == 0:\n",
    "                # It's ok to still append `temp` because the weight is reset as 0 in the\n",
    "                # later process.\n",
    "                artificial_measurements.append(temp)\n",
    "            else:\n",
    "                resized_image = cv2.resize(temp, (template_width, template_height))\n",
    "                # resized_image = np.asarray(Image.fromarray(temp).resize((template_width, template_height)))\n",
    "                \n",
    "                artificial_measurements.append(resized_image)\n",
    "\n",
    "        # NOTE: Add measurement noise? window += np.random.normal(loc=0, scale=self.std, size=window.shape).astype(np.uint8)\n",
    "        # Compute importance weights. Measuring similarity between each window and the template.\n",
    "        weights = []  # FIXME: Use array broadcasting\n",
    "        for m in artificial_measurements:\n",
    "            weights.append(self.measure_function(m, self.template.model))\n",
    "        weights = np.array(weights)\n",
    "\n",
    "        # NOTE: WE may want to clip weight here. eg, weight[weight<1e-3] = 0\n",
    "        # so that particles with very small weight can have zero chances.\n",
    "        # weights *= 1000\n",
    "        # weights[weights < 1e-3] = 0\n",
    "        # Normalize the weights\n",
    "        weights /= np.sum(weights)\n",
    "        return weights\n",
    "\n",
    "    def measure_function(self, window, template):\n",
    "        if window.shape != template.shape:\n",
    "            # NOTE: This statement must be here. It can not be put into distance function.\n",
    "            return 0\n",
    "        dist = self.distance_func(window, template)\n",
    "        weight = np.exp(-dist / (2 * self.std**2))\n",
    "\n",
    "        return weight\n",
    "\n",
    "    def update_template(self, frame, state):\n",
    "        x = state[0]\n",
    "        y = state[1]\n",
    "        w = state[2]\n",
    "        h = state[3]\n",
    "        start_x = int(x - w / 2)\n",
    "        start_y = int(y - h / 2)\n",
    "        end_x = int(start_x + w)\n",
    "        end_y = int(start_y + h)\n",
    "        best_model = frame[start_y:end_y, start_x:end_x, ...]\n",
    "\n",
    "        # Resize template to best state because the best state shrinks.\n",
    "        resized_template = cv2.resize(self.template.model, (w, h))\n",
    "        # resized_template = np.asarray(Image.fromarray(self.template.model).resize((w, h)))\n",
    "\n",
    "        if resized_template.shape != best_model.shape:\n",
    "            # Edge cases when best states are near boundries.\n",
    "            return\n",
    "\n",
    "        self.template.model = (\n",
    "            self.alpha * best_model + (1 - self.alpha) * resized_template\n",
    "        )\n",
    "        # self.template.model = self.template.model.astype(np.uint8)\n",
    "\n",
    "        # update x, y, w, h\n",
    "        self.template.x = x\n",
    "        self.template.y = y\n",
    "        self.template.w = w\n",
    "        self.template.h = h\n",
    "\n",
    "\n",
    "def mean_squared_error(window, template):\n",
    "    mse = np.sum(np.subtract(window, template, dtype=np.float64) ** 2)\n",
    "    mse /= float(window.shape[0] * window.shape[1])\n",
    "    return mse\n",
    "\n",
    "\n",
    "def cosine_similarity(window, template):\n",
    "    gray_window, gray_template = color.rgb2gray(window), color.rgb2gray(template)\n",
    "    # gray_window *= 255\n",
    "    # gray_template *= 255\n",
    "    dividend = np.sum(np.multiply(gray_window, gray_template))\n",
    "    divisor = np.multiply(\n",
    "        np.sqrt(np.sum(gray_window**2)), np.sqrt(np.sum(gray_template**2))\n",
    "    )\n",
    "    assert np.all(divisor != 0), f\"divisor has 0 in it. {divisor}\"\n",
    "    tmp = np.divide(dividend, divisor) + 1\n",
    "    assert tmp > 0\n",
    "    return 2 - tmp\n",
    "\n",
    "\n",
    "def chi_squared(window, template, num_bins=8):\n",
    "    hist1 = []\n",
    "    hist2 = []\n",
    "    for channel in range(window.shape[2]):\n",
    "        hist1_channel, _ = np.histogram(\n",
    "            window[:, :, channel], bins=num_bins, range=(0, 256)\n",
    "        )\n",
    "        hist2_channel, _ = np.histogram(\n",
    "            template[:, :, channel], bins=num_bins, range=(0, 256)\n",
    "        )\n",
    "        # Normalize the histograms does not work.\n",
    "        # hist1_channel = hist1_channel.astype(np.float64)\n",
    "        # hist2_channel = hist2_channel.astype(np.float64)\n",
    "        # hist1_channel /= hist1_channel.sum() + 1e-10\n",
    "        # hist2_channel /= hist2_channel.sum() + 1e-10\n",
    "        hist1.append(hist1_channel)\n",
    "        hist2.append(hist2_channel)\n",
    "    hist1 = np.array(hist1)\n",
    "    hist2 = np.array(hist2)\n",
    "\n",
    "    # Compute the Chi-Squared distance\n",
    "    chi_squared_distance = np.sum(((hist1 - hist2) ** 2) / (hist1 + hist2 + 1e-12))\n",
    "    return chi_squared_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `NaiveParticleFilter`\n",
    "\n",
    "In the design of the `NaiveParticleFilter`, we can discuss each aspect one by one:\n",
    "\n",
    "#### `num_states`:\n",
    "\n",
    "The `NaiveParticleFilter` consists of four states, as discussed in the preceding\n",
    "section.\n",
    "\n",
    "#### `state_boundry`:\n",
    "\n",
    "`state_boundry` represents the minimum and maximum values for each state, establishing\n",
    "the range within which the states must be constrained.\n",
    "\n",
    "#### `consensus`:\n",
    "\n",
    "The `consensus` parameter is a value ranging from 0 to 1. It is used to determine the\n",
    "proportion of particles with the highest weights that will contribute to certain\n",
    "decisions. It employs a form of minor meritocracy, where only particles with top-tier\n",
    "weights have a significant say in the decision-making process. For instance, if\n",
    "`consensus=0.05` and `num_particles=500`, then only 0.05 \\* 500 = 25 particles with the\n",
    "highest weights will be selected for decision-making.\n",
    "\n",
    "#### `upper_thres`:\n",
    "\n",
    "The `upper_thres` parameter comes into play after selecting the \"consensus\" particles\n",
    "with the highest weights. These particles must collectively contribute weights higher\n",
    "than the specified threshold, `upper_thres`. The rationale behind this is as follows:\n",
    "when dealing with a large number of particles (e.g., 100) randomly distributed across\n",
    "the state space, they might generate relatively random weights. In such cases, each\n",
    "particle's weight is approximately 0.01. While some particles may have higher weights by\n",
    "chance, it doesn't necessarily mean they are good estimates. These particles might have\n",
    "received higher weights purely by luck. Thus, the `upper_thres` serves to set a\n",
    "threshold ensuring that particles with higher weights significantly exceed this\n",
    "threshold, filtering out randomness and ensuring that the selected particles truly\n",
    "provide valuable estimations.\n",
    "\n",
    "However, if I apply a threshold on the normalized weights of particles, it can indeed\n",
    "lead to inaccuracies. This is because normalization ensures that the sum of the weights\n",
    "is equal to 1, effectively converting them into probabilities. By applying a threshold\n",
    "on these normalized weights, I might inadvertently alter the distribution of particles\n",
    "and the associated probabilities. This could result in selecting particles that would\n",
    "have otherwise been filtered out during the thresholding process.\n",
    "\n",
    "Applying the threshold in the measurement process before normalization might be a more\n",
    "reasonable approach in certain scenarios. This ensures that I am selecting particles\n",
    "based on the raw unnormalized weights, and the threshold doesn't interfere with the\n",
    "probabilistic interpretation of particle weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveParticleFilter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_states: int,\n",
    "        state_boundry: List[Tuple],\n",
    "        dynamic_model: DynamicModel,\n",
    "        measure_model: MeasurementModel,\n",
    "        num_particles=128,\n",
    "        consensus=0.05,  # percentage\n",
    "        upper_thres=0.3,  # percentage\n",
    "        lower_thres=0.1,  # percentage\n",
    "    ):\n",
    "        self.num_states = num_states\n",
    "        self.state_boundry = state_boundry\n",
    "        self.num_particles = num_particles\n",
    "        self.dynamic_model = dynamic_model\n",
    "        self.measure_model = measure_model\n",
    "\n",
    "        # Democracy here.\n",
    "        self.num_consensus_particles = int(self.num_particles * consensus)\n",
    "        self.upper_thres = upper_thres\n",
    "        # self.lower_thres = lower_thres\n",
    "\n",
    "        self.particles = np.zeros((num_particles, num_states))  # (N, 4): x, y, w, h\n",
    "        self.weights = np.ones(self.num_particles) / self.num_particles  # (N, ): weight\n",
    "\n",
    "        self.reset_particles()\n",
    "        self.state = self.particles[69, :]  # Initialize state randomly\n",
    "\n",
    "    def reset_particles(self):\n",
    "        for i in range(self.num_states):\n",
    "            lower_bound, upper_bound = self.state_boundry[i]\n",
    "            self.particles[:, i] = np.random.uniform(\n",
    "                lower_bound, upper_bound, self.num_particles\n",
    "            )\n",
    "        # Rest weights.\n",
    "        self.weights = np.ones(self.num_particles) / self.num_particles\n",
    "\n",
    "    def update(self, frame):\n",
    "        self.resample_particles()\n",
    "\n",
    "        self.dynamic_model.predict(self.particles, self.state_boundry)\n",
    "\n",
    "        self.weights = self.measure_model.measure(self.particles, frame)\n",
    "\n",
    "        # NOTE: MAYBE not always update states. Consider occlusion.\n",
    "        self.update_states()\n",
    "\n",
    "        sorted_indices = np.argsort(self.weights)\n",
    "        largest_indices = sorted_indices[-self.num_consensus_particles :]\n",
    "        if np.sum(self.weights[largest_indices]) > self.upper_thres:\n",
    "            self.measure_model.update_template(frame, self.state)\n",
    "\n",
    "    def resample_particles(self):\n",
    "        # Sample new particles indices using the distribution of the weights\n",
    "        j = np.random.choice(\n",
    "            np.arange(self.num_particles),\n",
    "            self.num_particles,\n",
    "            replace=True,\n",
    "            p=self.weights,\n",
    "        )\n",
    "        self.particles = np.array(self.particles[j])\n",
    "        assert self.particles.shape[0] == self.num_particles\n",
    "\n",
    "        # Constrain particles to be within boundries.\n",
    "        for i in range(self.num_states):\n",
    "            lower_bound, upper_bound = self.state_boundry[i]\n",
    "            np.clip(\n",
    "                self.particles[:, i], lower_bound, upper_bound, out=self.particles[:, i]\n",
    "            )\n",
    "\n",
    "        # Rest weights\n",
    "        self.weights = np.ones(self.num_particles) / self.num_particles\n",
    "\n",
    "    def update_states(self):\n",
    "        sorted_indices = np.argsort(self.weights)\n",
    "        largest_indices = sorted_indices[-self.num_consensus_particles :]\n",
    "        s = np.sum(self.particles[largest_indices, :], axis=0)\n",
    "        average = s / len(largest_indices)\n",
    "        self.state = average.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on a Simple Example\n",
    "\n",
    "With all the essential components defined, we are now ready to apply the particle filter\n",
    "to a simple example. The parameters chosen for this example are intentionally\n",
    "reasonable, avoiding any extreme values. In this particle filter, we employ only 128\n",
    "particles to track an object with four states. It's worth noting that I've followed the\n",
    "20/80 rule, setting the `consensus` to 0.2 and the `upper_thres` to 0.8. This rule implies\n",
    "that the sum of the top 20% of particles with the highest weights must exceed a\n",
    "probability of 0.8.\n",
    "\n",
    "The results are evident: the particles successfully and accurately track the position of\n",
    "the object, such as Romney's face, throughout the entire sequence.\n",
    "\n",
    "<center>\n",
    "<video src=\"images/romney.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_model = DynamicModel(std=(16, 16, 0.5, 0.5))\n",
    "measure_model = MeasurementModel(\n",
    "    std=16,\n",
    "    template=template,\n",
    "    distance_func=partial(mean_squared_error),\n",
    "    # distance_func=partial(chi_squared, num_bins=8),\n",
    "    # distance_func=partial(cosine_similarity), Not working\n",
    "    alpha=0.01,\n",
    ")\n",
    "state_boundry = [\n",
    "    (0, first_frame.shape[1] - 1),  # Width\n",
    "    (0, first_frame.shape[0] - 1),  # Height\n",
    "    (100, 105),  # Romney's head width\n",
    "    (125, 135),  # Romney's head height\n",
    "]\n",
    "tracker = NaiveParticleFilter(\n",
    "    num_states=4,\n",
    "    state_boundry=state_boundry,\n",
    "    dynamic_model=dynamic_model,\n",
    "    measure_model=measure_model,\n",
    "    num_particles=128,\n",
    "    consensus=0.20,  # percentage\n",
    "    upper_thres=0.80,  # percentage\n",
    "    # lower_thres=0.1,  # percentage\n",
    ")\n",
    "\n",
    "for i in range(1, president_video.shape[0]):\n",
    "    start_time = time.time()\n",
    "\n",
    "    frame = president_video[i, ...]\n",
    "\n",
    "    tracker.update(frame)\n",
    "\n",
    "    frame = visualize_particle_filter(\n",
    "        frame, tracker.particles, tracker.state, tracker.measure_model.template\n",
    "    )\n",
    "\n",
    "    delay = int(25 - (time.time() - start_time))\n",
    "    if cv2.waitKey(delay) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "    cv2.imshow(\"pres_debate\", frame[:, :, ::-1])  # RGB to BGR\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Occlusion and Depth Change\n",
    "\n",
    "In our initial example, tracking Mitt Romney's head was relatively straightforward. Now,\n",
    "we must address more realistic scenarios, such as scenes featuring occlusion and changes\n",
    "in object depth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_frame = blonde_video[0, ...]  # NOTE: uint8. 0-255.\n",
    "print(\"first_frame: \", first_frame.shape) # (360, 480, 3)\n",
    "x, y, w, h = 211, 36, 100, 293\n",
    "template = Template(first_frame, x, y, w, h)\n",
    "print(\n",
    "    \"template.model:\",\n",
    "    \"shape:\", template.model.shape,\n",
    "    \"type:\", template.model.dtype,\n",
    "    \"min:\", np.min(template.model),\n",
    "    \"max:\", np.max(template.model),\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 2))  # Adjust the figure size as needed\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "ax.imshow(template.model)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_computer_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
